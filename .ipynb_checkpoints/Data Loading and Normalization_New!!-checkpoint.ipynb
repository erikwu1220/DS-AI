{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from urllib.request import urlretrieve\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Imported because Roberto also did it.\n",
    "# from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dem, waterdepth, x_velocity, y_velocity data\n",
    "path = \"data/raw_datasets/\" \n",
    "\n",
    "# DEM files \n",
    "dem_tv = glob.glob(path + 'DEM_tra_val/*.txt')\n",
    "dem_t1 = glob.glob(path + 'DEM_test1/*.txt')\n",
    "dem_t2 = glob.glob(path + 'DEM_test2/*.txt')\n",
    "dem_t3 = glob.glob(path + 'DEM_test3/*.txt')\n",
    "                      \n",
    "# Water Depth files                      \n",
    "wd_tv = glob.glob(path + 'WD_tra_val/*.txt')\n",
    "wd_t1 = glob.glob(path + 'WD_test1/*.txt')\n",
    "wd_t2 = glob.glob(path + 'WD_test2/*.txt')\n",
    "wd_t3 = glob.glob(path + 'WD_test3/*.txt')\n",
    "                      \n",
    "# Velocity x files \n",
    "vx_tv = glob.glob(path + 'VX_tra_val/*.txt')\n",
    "vx_t1 = glob.glob(path + 'VX_test1/*.txt')\n",
    "vx_t2 = glob.glob(path + 'VX_test2/*.txt')\n",
    "vx_t3 = glob.glob(path + 'VX_test3/*.txt')\n",
    "                      \n",
    "# Velocity y files \n",
    "vy_tv = glob.glob(path + 'vy_tra_val/*.txt')\n",
    "vy_t1 = glob.glob(path + 'vy_test1/*.txt')\n",
    "vy_t2 = glob.glob(path + 'vy_test2/*.txt')\n",
    "vy_t3 = glob.glob(path + 'vy_test3/*.txt')\n",
    "                     \n",
    "dem_tra_val = []\n",
    "dem_tst1 = []\n",
    "dem_tst2 = []\n",
    "dem_tst3 = []\n",
    "                    \n",
    "wd_tra_val = []\n",
    "wd_tst1 = []\n",
    "wd_tst2 = []\n",
    "wd_tst3 = []\n",
    "\n",
    "vx_tra_val = []\n",
    "vx_tst1 = []\n",
    "vx_tst2 = []\n",
    "vx_tst3 = []\n",
    "\n",
    "vy_tra_val = []\n",
    "vy_tst1 = []\n",
    "vy_tst2 = []\n",
    "vy_tst3 = []\n",
    "\n",
    "for i in range(len(dem_tv)):\n",
    "    dem_tra_val.append(np.loadtxt(dem_tv[i]))\n",
    "    wd_tra_val.append(np.loadtxt(wd_tv[i]))\n",
    "    vx_tra_val.append(np.loadtxt(vx_tv[i]))\n",
    "    vy_tra_val.append(np.loadtxt(vy_tv[i]))\n",
    "                  \n",
    "for i in range(len(dem_t1)):\n",
    "    dem_tst1.append(np.loadtxt(dem_t1[i]))\n",
    "    wd_tst1.append(np.loadtxt(wd_t1[i]))\n",
    "    vx_tst1.append(np.loadtxt(vx_t1[i]))\n",
    "    vy_tst1.append(np.loadtxt(vy_t1[i]))\n",
    "\n",
    "for i in range(len(dem_t2)):\n",
    "    dem_tst2.append(np.loadtxt(dem_t2[i]))\n",
    "    wd_tst2.append(np.loadtxt(wd_t2[i]))\n",
    "    vx_tst2.append(np.loadtxt(vx_t2[i]))\n",
    "    vy_tst2.append(np.loadtxt(vy_t2[i]))\n",
    "                  \n",
    "for i in range(len(dem_t3)):\n",
    "    dem_tst3.append(np.loadtxt(dem_t3[i]))\n",
    "    wd_tst3.append(np.loadtxt(wd_t3[i]))\n",
    "    vx_tst3.append(np.loadtxt(vx_t3[i]))\n",
    "    vy_tst3.append(np.loadtxt(vy_t3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Extract DEM data at specific simulation\n",
    "# dem_sim = np.reshape(dem[sim][:,2],(number_grids,number_grids))\n",
    "\n",
    "# # Extract the water depth, vx, and vy value at specific time\n",
    "# wd_sim = np.reshape(wd[sim][t],(number_grids,number_grids))\n",
    "# vx_sim = np.reshape(vx[sim][t],(number_grids,number_grids))\n",
    "# vy_sim = np.reshape(vy[sim][t],(number_grids,number_grids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the train and validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vx_tra): 56\n",
      "len(vx_val): 24\n",
      "len(vx_tst1): 20\n",
      "len(vx_tst2): 21\n",
      "len(vx_tst3): 10\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset (in total 80) into following portion: training(70%) and validation(30%)\n",
    "# We already have the testing dataset\n",
    "\n",
    "# Water depth\n",
    "wd_tra, wd_val, iwd_tra, iwd_val = train_test_split(\n",
    "    wd_tra_val, np.arange(len(wd_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "# X velocity\n",
    "vx_tra, vx_val, ivx_tra, ivx_val = train_test_split(\n",
    "    vx_tra_val, np.arange(len(vx_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "# Y velocity\n",
    "vy_tra, vy_val, ivy_tra, ivy_val = train_test_split(\n",
    "    vy_tra_val, np.arange(len(vy_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"len(vx_tra): {len(wd_tra)}\")\n",
    "print(f\"len(vx_val): {len(wd_val)}\")\n",
    "print(f\"len(vx_tst1): {len(wd_tst1)}\")\n",
    "print(f\"len(vx_tst2): {len(wd_tst2)}\")\n",
    "print(f\"len(vx_tst3): {len(wd_tst3)}\")\n",
    "\n",
    "# Note that the form of the all the training, testing and validating dataset are \"a list containing multiple arrays\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking all the simulation for normalization \n",
    "# Note that each of the simulation contains t = 0 - 96\n",
    "wd_tra_arr = wd_tra[0]\n",
    "wd_val_arr = wd_val[0]\n",
    "wd_tst1_arr = wd_tst1[0]\n",
    "wd_tst2_arr = wd_tst2[0]\n",
    "wd_tst3_arr = wd_tst3[0]\n",
    "\n",
    "vx_tra_arr = vx_tra[0]\n",
    "vx_val_arr = vx_val[0]\n",
    "vx_tst1_arr = vx_tst1[0]\n",
    "vx_tst2_arr = vx_tst2[0]\n",
    "vx_tst3_arr = vx_tst3[0]\n",
    "\n",
    "vy_tra_arr = vy_tra[0]\n",
    "vy_val_arr = vy_val[0]\n",
    "vy_tst1_arr = vy_tst1[0]\n",
    "vy_tst2_arr = vy_tst2[0]\n",
    "vy_tst3_arr = vy_tst3[0]\n",
    "\n",
    "for i in range(1,len(wd_tra)):\n",
    "    wd_tra_arr = np.concatenate((wd_tra_arr, wd_tra[i]),axis=0)\n",
    "    vx_tra_arr = np.concatenate((vx_tra_arr, vx_tra[i]),axis=0)\n",
    "    vy_tra_arr = np.concatenate((vy_tra_arr, vy_tra[i]),axis=0)\n",
    "\n",
    "for i in range(1,len(wd_val)):\n",
    "    wd_val_arr = np.concatenate((wd_val_arr, wd_val[i]),axis=0)\n",
    "    vx_val_arr = np.concatenate((vx_val_arr, vx_val[i]),axis=0)\n",
    "    vy_val_arr = np.concatenate((vy_val_arr, vy_val[i]),axis=0)\n",
    "    \n",
    "for i in range(1,len(wd_tst1)):\n",
    "    wd_tst1_arr = np.concatenate((wd_tst1_arr, wd_tst1[i]),axis=0)\n",
    "    vx_tst1_arr = np.concatenate((vx_tst1_arr, vx_tst1[i]),axis=0)\n",
    "    vy_tst1_arr = np.concatenate((vy_tst1_arr, vy_tst1[i]),axis=0)\n",
    "    \n",
    "for i in range(1,len(wd_tst2)):\n",
    "    wd_tst2_arr = np.concatenate((wd_tst2_arr, wd_tst2[i]),axis=0)\n",
    "    vx_tst2_arr = np.concatenate((vx_tst2_arr, vx_tst2[i]),axis=0)\n",
    "    vy_tst2_arr = np.concatenate((vy_tst2_arr, vy_tst2[i]),axis=0)\n",
    "    \n",
    "for i in range(1,len(wd_tst3)):\n",
    "    wd_tst3_arr = np.concatenate((wd_tst3_arr, wd_tst3[i]),axis=0)\n",
    "    vx_tst3_arr = np.concatenate((vx_tst3_arr, vx_tst3[i]),axis=0)\n",
    "    vy_tst3_arr = np.concatenate((vy_tst3_arr, vy_tst3[i]),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here use the Min-max normalization to normalize the water depth over the entire simulation sequence (0-130)\n",
    "def scale_sequences(X,scaler=None,scaler_type='minmax'):\n",
    "    \"\"\"\n",
    "    Uses a minmax scaler to transform sequences. The scaler is created if no scaler is passed as argument.\n",
    "    Adapted from exercise notebook on drinking water demand.\n",
    "    \n",
    "    The input parameter X is a two-dimensional array.\n",
    "    \"\"\"\n",
    "    \n",
    "    Xshape=X.shape\n",
    "    if scaler:\n",
    "        X = scaler.transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X\n",
    "    else:\n",
    "        if scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise Exception(\"Type of scikit-learn scaler not supported. Choose 'standard' or 'minmax.\")\n",
    "        X = scaler.fit_transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X, scaler\n",
    "    \n",
    "def denormalize(image_tensor, mean, std):\n",
    "    # Denormalize the image\n",
    "    denorm_img = image_tensor * std[:, None, None] + mean[:, None, None]\n",
    "    # Clip values to be between 0 and 1\n",
    "    denorm_img = denorm_img.clip(0, 1)\n",
    "    return denorm_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of waterdepth and velocity\n",
    "wd_tra_scale, wd_tra_scaler = scale_sequences(wd_tra_arr ,scaler=None,scaler_type='minmax')\n",
    "wd_val_scale, wd_val_scaler = scale_sequences(wd_val_arr ,scaler=None,scaler_type='minmax')\n",
    "wd_tst1_scale, wd_tst1_scaler = scale_sequences(wd_tst1_arr ,scaler=None,scaler_type='minmax')\n",
    "wd_tst2_scale, wd_tst2_scaler = scale_sequences(wd_tst2_arr ,scaler=None,scaler_type='minmax')\n",
    "wd_tst3_scale, wd_tst3_scaler = scale_sequences(wd_tst3_arr ,scaler=None,scaler_type='minmax')\n",
    "\n",
    "vx_tra_scale, vx_tra_scaler = scale_sequences(vx_tra_arr ,scaler=None,scaler_type='minmax')\n",
    "vx_val_scale, vx_val_scaler = scale_sequences(vx_val_arr ,scaler=None,scaler_type='minmax')\n",
    "vx_tst1_scale, vx_tst1_scaler = scale_sequences(vx_tst1_arr ,scaler=None,scaler_type='minmax')\n",
    "vx_tst2_scale, vx_tst2_scaler = scale_sequences(vx_tst2_arr ,scaler=None,scaler_type='minmax')\n",
    "vx_tst3_scale, vx_tst3_scaler = scale_sequences(vx_tst3_arr ,scaler=None,scaler_type='minmax')\n",
    "\n",
    "vy_tra_scale, vy_tra_scaler = scale_sequences(vy_tra_arr ,scaler=None,scaler_type='minmax')\n",
    "vy_val_scale, vy_val_scaler = scale_sequences(vy_val_arr ,scaler=None,scaler_type='minmax')\n",
    "vy_tst1_scale, vy_tst1_scaler = scale_sequences(vy_tst1_arr ,scaler=None,scaler_type='minmax')\n",
    "vy_tst2_scale, vy_tst2_scaler = scale_sequences(vy_tst2_arr ,scaler=None,scaler_type='minmax')\n",
    "vy_tst3_scale, vy_tst3_scaler = scale_sequences(vy_tst3_arr ,scaler=None,scaler_type='minmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalization all the data, split them into the original form (131 simulations with t = 0-96 of each)\n",
    "wd_tra_norm = np.vsplit(wd_tra_scale,len(wd_tra))\n",
    "wd_val_norm = np.vsplit(wd_val_scale,len(wd_val))\n",
    "wd_tst1_norm = np.vsplit(wd_tst1_scale,len(wd_tst1))\n",
    "wd_tst2_norm = np.vsplit(wd_tst2_scale,len(wd_tst2))\n",
    "wd_tst3_norm = np.vsplit(wd_tst3_scale,len(wd_tst3))\n",
    "\n",
    "vx_tra_norm = np.vsplit(vx_tra_scale,len(vx_tra))\n",
    "vx_val_norm = np.vsplit(vx_val_scale,len(vx_val))\n",
    "vx_tst1_norm = np.vsplit(vx_tst1_scale,len(vx_tst1))\n",
    "vx_tst2_norm = np.vsplit(vx_tst2_scale,len(vx_tst2))\n",
    "vx_tst3_norm = np.vsplit(vx_tst3_scale,len(vx_tst3))\n",
    "\n",
    "vy_tra_norm = np.vsplit(vy_tra_scale,len(vy_tra))\n",
    "vy_val_norm = np.vsplit(vy_val_scale,len(vy_val))\n",
    "vy_tst1_norm = np.vsplit(vy_tst1_scale,len(vy_tst1))\n",
    "vy_tst2_norm = np.vsplit(vy_tst2_scale,len(vy_tst2))\n",
    "vy_tst3_norm = np.vsplit(vy_tst3_scale,len(vy_tst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert all the data to np.array\n",
    "\n",
    "wd_tra_norm = np.array(wd_tra_norm)\n",
    "wd_val_norm = np.array(wd_val_norm)\n",
    "wd_tst1_norm = np.array(wd_tst1_norm)\n",
    "wd_tst2_norm = np.array(wd_tst2_norm)\n",
    "wd_tst3_norm = np.array(wd_tst3_norm)\n",
    "\n",
    "vx_tra_norm = np.array(vx_tra_norm)\n",
    "vx_val_norm = np.array(vx_val_norm)\n",
    "vx_tst1_norm = np.array(vx_tst1_norm)\n",
    "vx_tst2_norm = np.array(vx_tst2_norm)\n",
    "vx_tst3_norm = np.array(vx_tst3_norm)\n",
    "\n",
    "vy_tra_norm = np.array(wd_tra_norm)\n",
    "vy_val_norm = np.array(vy_val_norm)\n",
    "vy_tst1_norm = np.array(vy_tst1_norm)\n",
    "vy_tst2_norm = np.array(vy_tst2_norm)\n",
    "vy_tst3_norm = np.array(vy_tst3_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tra = [wd_tra_norm, vx_tra_norm, vy_tra_norm]\n",
    "datalist_tra = ['wd_tra_norm', ' vx_tra_norm', 'vy_tra_norm']\n",
    "\n",
    "\n",
    "# [wd_tra_norm, wd_val_norm, wd_tst1_norm, wd_tst2_norm ,wd_tst3_norm, \n",
    "#            vx_tra_norm, vx_val_norm, vx_tst1_norm, vx_tst2_norm, vx_tst3_norm, \n",
    "#            vy_tra_norm, vy_val_norm, vy_tst1_norm, vy_tst2_norm, vy_tst3_norm]\n",
    "# dataset_val = \n",
    "# dataset_tst = \n",
    "# datalist = ['wd_tra_norm', 'wd_val_norm', 'wd_tst1_norm', 'wd_tst2_norm', 'wd_tst3_norm', \n",
    "#            'vx_tra_norm', 'vx_val_norm', 'vx_tst1_norm', 'vx_tst2_norm', 'vx_tst3_norm', \n",
    "#            'vy_tra_norm', 'vy_val_norm', 'vy_tst1_norm', 'vy_tst2_norm', 'vy_tst3_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagi8\\Documents\\GitHub\\data\\processed_data/\n"
     ]
    }
   ],
   "source": [
    "path = pathlib.Path().resolve()\n",
    "proj_dir = str(path.parent)\n",
    "save_path = proj_dir + \"\\data\\processed_data/\"\n",
    "\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_tra' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ds, fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mdataset_tra\u001b[49m, datalist_tra):\n\u001b[0;32m      2\u001b[0m     np\u001b[38;5;241m.\u001b[39msavetxt(save_path\u001b[38;5;241m+\u001b[39mfname, ds, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_tra' is not defined"
     ]
    }
   ],
   "source": [
    "for ds, fname in zip(dataset_tra, datalist_tra):\n",
    "    np.savetxt(save_path+fname, ds, delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wdtrain_dataset = TensorDataset(torch.tensor(wd_tra, dtype=torch.float32))\n",
    "# wdval_dataset = TensorDataset(torch.tensor(wd_val, dtype=torch.float32))\n",
    "# wdtest_dataset = TensorDataset(torch.tensor(wd_tst, dtype=torch.float32))\n",
    "\n",
    "# vxtrain_dataset = TensorDataset(torch.tensor(vx_tra, dtype=torch.float32))\n",
    "# vxval_dataset = TensorDataset(torch.tensor(vx_val, dtype=torch.float32))\n",
    "# vxtest_dataset = TensorDataset(torch.tensor(vx_tst, dtype=torch.float32))\n",
    "\n",
    "# vytrain_dataset = TensorDataset(torch.tensor(vy_tra, dtype=torch.float32))\n",
    "# vyval_dataset = TensorDataset(torch.tensor(vy_val, dtype=torch.float32))\n",
    "# vytest_dataset = TensorDataset(torch.tensor(vy_tst, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_pyg(graph, pos, DEM, WD, VX, VY):\n",
    "#     '''\n",
    "#     Converts a graph or mesh into a PyTorch Geometric Data type \n",
    "#     Then, add position, DEM, and water variables to data object.\n",
    "#     Adapted from https://github.com/RBTV1/SWE-GNN-paper-repository-/blob/main/database/graph_creation.py\n",
    "#     '''\n",
    "#     DEM = DEM.reshape(-1)\n",
    "\n",
    "#     edge_index = torch.LongTensor(list(graph.edges)).t().contiguous()\n",
    "#     row, col = edge_index\n",
    "\n",
    "#     data = Data()\n",
    "\n",
    "#     delta_DEM = torch.FloatTensor(DEM[col]-DEM[row])\n",
    "#     coords = torch.FloatTensor(get_coords(pos))\n",
    "#     edge_relative_distance = coords[col] - coords[row]\n",
    "#     edge_distance = torch.norm(edge_relative_distance, dim=1)\n",
    "#     edge_slope = delta_DEM/edge_distance\n",
    "\n",
    "#     data.edge_index = edge_index\n",
    "#     data.edge_distance = edge_distance\n",
    "#     data.edge_slope = edge_slope\n",
    "#     data.edge_relative_distance = edge_relative_distance\n",
    "\n",
    "#     data.num_nodes = graph.number_of_nodes()\n",
    "#     data.pos = torch.tensor(list(pos.values()))\n",
    "#     data.DEM = torch.FloatTensor(DEM)\n",
    "#     data.WD = torch.FloatTensor(WD.T)\n",
    "#     data.VX = torch.FloatTensor(VX.T)\n",
    "#     data.VY = torch.FloatTensor(VY.T)\n",
    "        \n",
    "#     return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
