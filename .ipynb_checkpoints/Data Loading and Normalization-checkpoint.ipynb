{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Imported because Roberto also did it.\n",
    "# from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = 42\n",
    "number_grids = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a random model\n",
    "save_folder = \"data/raw_datasets/\"\n",
    "\n",
    "topo = np.loadtxt(f\"{save_folder}\\\\DEM\\\\DEM_{sim}.txt\")[:, 2].reshape(number_grids,number_grids)\n",
    "vals = np.loadtxt(f\"{save_folder}\\\\WD\\\\WD_{sim}.txt\").reshape(-1,number_grids,number_grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.     ,  0.03362,  0.02552, ..., -0.07375,  0.05459,  0.14767],\n",
       "       [-0.12979, -0.09082, -0.06958, ...,  0.00577,  0.13852,  0.23505],\n",
       "       [-0.16733, -0.1261 , -0.08684, ...,  0.09362,  0.22339,  0.31521],\n",
       "       ...,\n",
       "       [ 0.39018,  0.38217,  0.29131, ..., -0.48051, -0.41116, -0.25394],\n",
       "       [ 0.40054,  0.34883,  0.25079, ..., -0.47472, -0.39348, -0.21938],\n",
       "       [ 0.30491,  0.2184 ,  0.13165, ..., -0.44601, -0.37985, -0.21325]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(topo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pyg(graph, pos, DEM, WD, VX, VY):\n",
    "    '''\n",
    "    Converts a graph or mesh into a PyTorch Geometric Data type \n",
    "    Then, add position, DEM, and water variables to data object.\n",
    "    Adapted from https://github.com/RBTV1/SWE-GNN-paper-repository-/blob/main/database/graph_creation.py\n",
    "    '''\n",
    "    DEM = DEM.reshape(-1)\n",
    "\n",
    "    edge_index = torch.LongTensor(list(graph.edges)).t().contiguous()\n",
    "    row, col = edge_index\n",
    "\n",
    "    data = Data()\n",
    "\n",
    "    delta_DEM = torch.FloatTensor(DEM[col]-DEM[row])\n",
    "    coords = torch.FloatTensor(get_coords(pos))\n",
    "    edge_relative_distance = coords[col] - coords[row]\n",
    "    edge_distance = torch.norm(edge_relative_distance, dim=1)\n",
    "    edge_slope = delta_DEM/edge_distance\n",
    "\n",
    "    data.edge_index = edge_index\n",
    "    data.edge_distance = edge_distance\n",
    "    data.edge_slope = edge_slope\n",
    "    data.edge_relative_distance = edge_relative_distance\n",
    "\n",
    "    data.num_nodes = graph.number_of_nodes()\n",
    "    data.pos = torch.tensor(list(pos.values()))\n",
    "    data.DEM = torch.FloatTensor(DEM)\n",
    "    data.WD = torch.FloatTensor(WD.T)\n",
    "    data.VX = torch.FloatTensor(VX.T)\n",
    "    data.VY = torch.FloatTensor(VY.T)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_sequences(X,scaler=None,scaler_type='standard'):\n",
    "    \"\"\"\n",
    "    Uses a standard scaler to transform sequences. The scaler is created if no scaler is passed as argument.\n",
    "    Adapted from exercise notebook on drinking water demand.\n",
    "    \"\"\"\n",
    "    Xshape=X.shape\n",
    "    if scaler:\n",
    "        X = scaler.transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X\n",
    "    else:\n",
    "        if scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise Exception(\"Type of scikit-learn scaler not supported. Choose 'standard' or 'minmax.\")\n",
    "        X = scaler.fit_transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep track of indexes of train and validation.\n",
    "X_tra, X_tst, Y_tra, Y_tst, ix_tra, ix_tst = train_test_split(\n",
    "    X, Y, np.arange(X.shape[0]), test_size=0.30, shuffle=True, random_state=42)\n",
    "\n",
    "# Split the existing test dataset into validation and test sets (50/50 split)\n",
    "X_val, X_tst, Y_val, Y_tst, ix_val, ix_tst = train_test_split(\n",
    "    X_tst, Y_tst, ix_tst, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"X_tra.shape: {X_tra.shape}, Y_tra.shape: {Y_tra.shape}\")\n",
    "print(f\"X_val.shape: {X_val.shape}, Y_val.shape: {Y_val.shape}\")\n",
    "print(f\"X_tst.shape: {X_tst.shape}, Y_tst.shape: {Y_tst.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block is from the landuse CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(file_paths, transform, folder_path=os.path.join(dataset_folder, 'Images/Images/')):\n",
    "    images = []\n",
    "    for file_path in tqdm(file_paths, desc='Loading images'):\n",
    "        # Load the image\n",
    "        with Image.open(folder_path+file_path) as img:\n",
    "            # Convert image to RGB if it's not and apply the same basic transformations\n",
    "            img = img.convert('RGB')\n",
    "            img = transform(img)\n",
    "            images.append(img)\n",
    "    return torch.stack(images)\n",
    "\n",
    "def calculate_mean_std(stacked_images):\n",
    "    # Mean and std are calculated across the height and width dimensions (2 and 3)\n",
    "    mean = stacked_images.view(stacked_images.size(0), stacked_images.size(1), -1).mean(dim=2).mean(dim=0)\n",
    "    std = stacked_images.view(stacked_images.size(0), stacked_images.size(1), -1).std(dim=2).mean(dim=0)\n",
    "    return mean, std\n",
    "\n",
    "# Basic image transformations to load the training dataset\n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load images for the mean/std calculation\n",
    "original_train_images = load_images(train_paths, basic_transform)\n",
    "\n",
    "# Calculate mean and std\n",
    "mean, std = calculate_mean_std(original_train_images)\n",
    "\n",
    "# Create the normalization transform using mean and std\n",
    "normalize_transform = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "# Apply the normalization to each original training image\n",
    "original_train_images = torch.stack([normalize_transform(image) for image in original_train_images])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
