{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os.path\n",
    "import glob\n",
    "from urllib.request import urlretrieve\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Imported because Roberto also did it.\n",
    "# from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagi8\\Documents\\GitHub\\DS-AI\\data/raw_datasets\n",
      "C:\\Users\\sagi8\\Documents\\GitHub\\DS-AI\\data/augmented_data\n"
     ]
    }
   ],
   "source": [
    "path = pathlib.Path().resolve()\n",
    "proj_dir = str(path.parent)\n",
    "path_raw = proj_dir + \"\\data/raw_datasets\"\n",
    "path_aug = proj_dir + \"\\data/augmented_data\"\n",
    "print(path_raw)\n",
    "print(path_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Reading me!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the augmented validation and training data. (from Patrick)\\\n",
    "Load the original testing data (Raw dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load waterdepth, x_velocity, y_velocity data\n",
    "\n",
    "# DEM\n",
    "dem_tv = glob.glob(path_aug + '/DEM/*.txt')\n",
    "dem_t1 = glob.glob(path_raw + '/test1/DEM/*.txt')\n",
    "dem_t2 = glob.glob(path_raw + '/test2/DEM/*.txt')\n",
    "dem_t3 = glob.glob(path_raw + '/test3/DEM/*.txt')\n",
    "\n",
    "# Water Depth files              \n",
    "wd_tv = glob.glob(path_aug + '/WD/*.txt')\n",
    "wd_t1 = glob.glob(path_raw + '/test1/WD/*.txt')\n",
    "wd_t2 = glob.glob(path_raw + '/test2/WD/*.txt')\n",
    "wd_t3 = glob.glob(path_raw + '/test3/WD/*.txt')\n",
    "                      \n",
    "# Velocity x files \n",
    "vx_tv = glob.glob(path_aug + '/VX/*.txt')\n",
    "vx_t1 = glob.glob(path_raw + '/test1/VX/*.txt')\n",
    "vx_t2 = glob.glob(path_raw + '/test2/VX/*.txt')\n",
    "vx_t3 = glob.glob(path_raw + '/test3/VX/*.txt')\n",
    "                      \n",
    "# Velocity y files \n",
    "vy_tv = glob.glob(path_aug + '/VY/*.txt')\n",
    "vy_t1 = glob.glob(path_raw + '/test1/VY/*.txt')\n",
    "vy_t2 = glob.glob(path_raw + '/test2/VY/*.txt')\n",
    "vy_t3 = glob.glob(path_raw + '/test3/VY/*.txt')\n",
    "                     \n",
    "dem_tra_val = []\n",
    "dem_tst1 = []\n",
    "dem_tst2 = []\n",
    "dem_tst3 = []\n",
    "                    \n",
    "# wd_tra_val = []\n",
    "# wd_tst1 = []\n",
    "# wd_tst2 = []\n",
    "# wd_tst3 = []\n",
    "\n",
    "# vx_tra_val = []\n",
    "# vx_tst1 = []\n",
    "# vx_tst2 = []\n",
    "# vx_tst3 = []\n",
    "\n",
    "# vy_tra_val = []\n",
    "# vy_tst1 = []\n",
    "# vy_tst2 = []\n",
    "# vy_tst3 = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(wd_tv)):\n",
    "    dem_tra_val.append(np.loadtxt(dem_tv[i]))\n",
    "#     wd_tra_val.append(np.loadtxt(wd_tv[i]))\n",
    "#     vx_tra_val.append(np.loadtxt(vx_tv[i]))\n",
    "#     vy_tra_val.append(np.loadtxt(vy_tv[i]))\n",
    "                  \n",
    "for i in range(len(wd_t1)):\n",
    "    dem_tst1.append(np.loadtxt(dem_t1[i]))\n",
    "#     wd_tst1.append(np.loadtxt(wd_t1[i]))\n",
    "#     vx_tst1.append(np.loadtxt(vx_t1[i]))\n",
    "#     vy_tst1.append(np.loadtxt(vy_t1[i]))\n",
    "\n",
    "for i in range(len(wd_t2)):\n",
    "    dem_tst2.append(np.loadtxt(dem_t2[i]))\n",
    "#     wd_tst2.append(np.loadtxt(wd_t2[i]))\n",
    "#     vx_tst2.append(np.loadtxt(vx_t2[i]))\n",
    "#     vy_tst2.append(np.loadtxt(vy_t2[i]))\n",
    "                  \n",
    "for i in range(len(wd_t3)):\n",
    "    dem_tst3.append(np.loadtxt(dem_t3[i]))\n",
    "#     wd_tst3.append(np.loadtxt(wd_t3[i]))\n",
    "#     vx_tst3.append(np.loadtxt(vx_t3[i]))\n",
    "#     vy_tst3.append(np.loadtxt(vy_t3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4096, 3)\n",
      "<class 'list'>\n",
      "(10, 16384, 3)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(dem_tra_val))\n",
    "print(type(dem_tra_val))\n",
    "\n",
    "print(np.shape(dem_tst3))\n",
    "print(type(dem_tst3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the train and validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vx_tra): 280\n",
      "len(vx_val): 120\n",
      "len(vx_tst1): 20\n",
      "len(vx_tst2): 21\n",
      "len(vx_tst3): 10\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset (in total 80) into following portion: training(70%) and validation(30%)\n",
    "# We already have the testing dataset\n",
    "# DEM\n",
    "dem_tra, dem_val, idem_tra, idem_val = train_test_split(\n",
    "    dem_tra_val, np.arange(len(dem_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "# # Water depth\n",
    "# wd_tra, wd_val, iwd_tra, iwd_val = train_test_split(\n",
    "#     wd_tra_val, np.arange(len(wd_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "# # X velocity\n",
    "# vx_tra, vx_val, ivx_tra, ivx_val = train_test_split(\n",
    "#     vx_tra_val, np.arange(len(vx_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "# # Y velocity\n",
    "# vy_tra, vy_val, ivy_tra, ivy_val = train_test_split(\n",
    "#     vy_tra_val, np.arange(len(vy_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"len(vx_tra): {len(dem_tra)}\")\n",
    "print(f\"len(vx_val): {len(dem_val)}\")\n",
    "print(f\"len(vx_tst1): {len(dem_tst1)}\")\n",
    "print(f\"len(vx_tst2): {len(dem_tst2)}\")\n",
    "print(f\"len(vx_tst3): {len(dem_tst3)}\")\n",
    "\n",
    "# Note that the form of the all the training, testing and validating dataset are \"a list containing multiple arrays\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking all the simulation for normalization \n",
    "# Note that each of the simulation contains t = 0 - 96\n",
    "dem_tra_arr = dem_tra[0]\n",
    "dem_val_arr = dem_val[0]\n",
    "dem_tst1_arr = dem_tst1[0]\n",
    "dem_tst2_arr = dem_tst2[0]\n",
    "dem_tst3_arr = dem_tst3[0]\n",
    "\n",
    "# wd_tra_arr = wd_tra[0]\n",
    "# wd_val_arr = wd_val[0]\n",
    "# wd_tst1_arr = wd_tst1[0]\n",
    "# wd_tst2_arr = wd_tst2[0]\n",
    "# wd_tst3_arr = wd_tst3[0]\n",
    "\n",
    "# vx_tra_arr = vx_tra[0]\n",
    "# vx_val_arr = vx_val[0]\n",
    "# vx_tst1_arr = vx_tst1[0]\n",
    "# vx_tst2_arr = vx_tst2[0]\n",
    "# vx_tst3_arr = vx_tst3[0]\n",
    "\n",
    "# vy_tra_arr = vy_tra[0]\n",
    "# vy_val_arr = vy_val[0]\n",
    "# vy_tst1_arr = vy_tst1[0]\n",
    "# vy_tst2_arr = vy_tst2[0]\n",
    "# vy_tst3_arr = vy_tst3[0]\n",
    "\n",
    "for i in range(1,len(dem_tra)):\n",
    "    dem_tra_arr = np.concatenate((dem_tra_arr, dem_tra[i]),axis=0)\n",
    "#     wd_tra_arr = np.concatenate((wd_tra_arr, wd_tra[i]),axis=0)\n",
    "#     vx_tra_arr = np.concatenate((vx_tra_arr, vx_tra[i]),axis=0)\n",
    "#     vy_tra_arr = np.concatenate((vy_tra_arr, vy_tra[i]),axis=0)\n",
    "\n",
    "for i in range(1,len(dem_val)):\n",
    "    dem_val_arr = np.concatenate((dem_val_arr, dem_val[i]),axis=0)\n",
    "#     wd_val_arr = np.concatenate((wd_val_arr, wd_val[i]),axis=0)\n",
    "#     vx_val_arr = np.concatenate((vx_val_arr, vx_val[i]),axis=0)\n",
    "#     vy_val_arr = np.concatenate((vy_val_arr, vy_val[i]),axis=0)\n",
    "    \n",
    "for i in range(1,len(dem_tst1)):\n",
    "    dem_tst1_arr = np.concatenate((dem_tst1_arr, dem_tst1[i]),axis=0)\n",
    "#     wd_tst1_arr = np.concatenate((wd_tst1_arr, wd_tst1[i]),axis=0)\n",
    "#     vx_tst1_arr = np.concatenate((vx_tst1_arr, vx_tst1[i]),axis=0)\n",
    "#     vy_tst1_arr = np.concatenate((vy_tst1_arr, vy_tst1[i]),axis=0)\n",
    "    \n",
    "for i in range(1,len(dem_tst2)):\n",
    "    dem_tst2_arr = np.concatenate((dem_tst2_arr, dem_tst2[i]),axis=0)\n",
    "#     wd_tst2_arr = np.concatenate((wd_tst2_arr, wd_tst2[i]),axis=0)\n",
    "#     vx_tst2_arr = np.concatenate((vx_tst2_arr, vx_tst2[i]),axis=0)\n",
    "#     vy_tst2_arr = np.concatenate((vy_tst2_arr, vy_tst2[i]),axis=0)\n",
    "    \n",
    "for i in range(1,len(dem_tst3)):\n",
    "    dem_tst3_arr = np.concatenate((dem_tst3_arr, dem_tst3[i]),axis=0)\n",
    "#     wd_tst3_arr = np.concatenate((wd_tst3_arr, wd_tst3[i]),axis=0)\n",
    "#     vx_tst3_arr = np.concatenate((vx_tst3_arr, vx_tst3[i]),axis=0)\n",
    "#     vy_tst3_arr = np.concatenate((vy_tst3_arr, vy_tst3[i]),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here use the Min-max normalization to normalize the water depth over the entire simulation sequence (0-130)\n",
    "def scale_sequences(X,scaler=None,scaler_type='minmax'):\n",
    "    \"\"\"\n",
    "    Uses a minmax scaler to transform sequences. The scaler is created if no scaler is passed as argument.\n",
    "    Adapted from exercise notebook on drinking water demand.\n",
    "    \n",
    "    The input parameter X is a two-dimensional array.\n",
    "    \"\"\"\n",
    "    \n",
    "    Xshape=X.shape\n",
    "    if scaler:\n",
    "        X = scaler.transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X\n",
    "    else:\n",
    "        if scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise Exception(\"Type of scikit-learn scaler not supported. Choose 'standard' or 'minmax.\")\n",
    "        X = scaler.fit_transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X, scaler\n",
    "    \n",
    "def denormalize(image_tensor, mean, std):\n",
    "    # Denormalize the image\n",
    "    denorm_img = image_tensor * std[:, None, None] + mean[:, None, None]\n",
    "    # Clip values to be between 0 and 1\n",
    "    denorm_img = denorm_img.clip(0, 1)\n",
    "    return denorm_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of waterdepth and velocity\n",
    "\n",
    "dem_tra_scale, dem_tra_scaler = scale_sequences(dem_tra_arr ,scaler=None,scaler_type='minmax')\n",
    "dem_val_scale, dem_val_scaler = scale_sequences(dem_val_arr ,scaler=None,scaler_type='minmax')\n",
    "dem_tst1_scale, dem_tst1_scaler = scale_sequences(dem_tst1_arr ,scaler=None,scaler_type='minmax')\n",
    "dem_tst2_scale, dem_tst2_scaler = scale_sequences(dem_tst2_arr ,scaler=None,scaler_type='minmax')\n",
    "dem_tst3_scale, dem_tst3_scaler = scale_sequences(dem_tst3_arr ,scaler=None,scaler_type='minmax')\n",
    "\n",
    "# wd_tra_scale, wd_tra_scaler = scale_sequences(wd_tra_arr ,scaler=None,scaler_type='minmax')\n",
    "# wd_val_scale, wd_val_scaler = scale_sequences(wd_val_arr ,scaler=None,scaler_type='minmax')\n",
    "# wd_tst1_scale, wd_tst1_scaler = scale_sequences(wd_tst1_arr ,scaler=None,scaler_type='minmax')\n",
    "# wd_tst2_scale, wd_tst2_scaler = scale_sequences(wd_tst2_arr ,scaler=None,scaler_type='minmax')\n",
    "# wd_tst3_scale, wd_tst3_scaler = scale_sequences(wd_tst3_arr ,scaler=None,scaler_type='minmax')\n",
    "\n",
    "# vx_tra_scale, vx_tra_scaler = scale_sequences(vx_tra_arr ,scaler=None,scaler_type='minmax')\n",
    "# vx_val_scale, vx_val_scaler = scale_sequences(vx_val_arr ,scaler=None,scaler_type='minmax')\n",
    "# vx_tst1_scale, vx_tst1_scaler = scale_sequences(vx_tst1_arr ,scaler=None,scaler_type='minmax')\n",
    "# vx_tst2_scale, vx_tst2_scaler = scale_sequences(vx_tst2_arr ,scaler=None,scaler_type='minmax')\n",
    "# vx_tst3_scale, vx_tst3_scaler = scale_sequences(vx_tst3_arr ,scaler=None,scaler_type='minmax')\n",
    "\n",
    "# vy_tra_scale, vy_tra_scaler = scale_sequences(vy_tra_arr ,scaler=None,scaler_type='minmax')\n",
    "# vy_val_scale, vy_val_scaler = scale_sequences(vy_val_arr ,scaler=None,scaler_type='minmax')\n",
    "# vy_tst1_scale, vy_tst1_scaler = scale_sequences(vy_tst1_arr ,scaler=None,scaler_type='minmax')\n",
    "# vy_tst2_scale, vy_tst2_scaler = scale_sequences(vy_tst2_arr ,scaler=None,scaler_type='minmax')\n",
    "# vy_tst3_scale, vy_tst3_scaler = scale_sequences(vy_tst3_arr ,scaler=None,scaler_type='minmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After normalization all the data, split them into the original form (131 simulations with t = 0-96 of each)\n",
    "\n",
    "dem_tra_norm = np.vsplit(dem_tra_scale,len(dem_tra))\n",
    "dem_val_norm = np.vsplit(dem_val_scale,len(dem_val))\n",
    "dem_tst1_norm = np.vsplit(dem_tst1_scale,len(dem_tst1))\n",
    "dem_tst2_norm = np.vsplit(dem_tst2_scale,len(dem_tst2))\n",
    "dem_tst3_norm = np.vsplit(dem_tst3_scale,len(dem_tst3))\n",
    "\n",
    "# wd_tra_norm = np.vsplit(wd_tra_scale,len(wd_tra))\n",
    "# wd_val_norm = np.vsplit(wd_val_scale,len(wd_val))\n",
    "# wd_tst1_norm = np.vsplit(wd_tst1_scale,len(wd_tst1))\n",
    "# wd_tst2_norm = np.vsplit(wd_tst2_scale,len(wd_tst2))\n",
    "# wd_tst3_norm = np.vsplit(wd_tst3_scale,len(wd_tst3))\n",
    "\n",
    "# vx_tra_norm = np.vsplit(vx_tra_scale,len(vx_tra))\n",
    "# vx_val_norm = np.vsplit(vx_val_scale,len(vx_val))\n",
    "# vx_tst1_norm = np.vsplit(vx_tst1_scale,len(vx_tst1))\n",
    "# vx_tst2_norm = np.vsplit(vx_tst2_scale,len(vx_tst2))\n",
    "# vx_tst3_norm = np.vsplit(vx_tst3_scale,len(vx_tst3))\n",
    "\n",
    "# vy_tra_norm = np.vsplit(vy_tra_scale,len(vy_tra))\n",
    "# vy_val_norm = np.vsplit(vy_val_scale,len(vy_val))\n",
    "# vy_tst1_norm = np.vsplit(vy_tst1_scale,len(vy_tst1))\n",
    "# vy_tst2_norm = np.vsplit(vy_tst2_scale,len(vy_tst2))\n",
    "# vy_tst3_norm = np.vsplit(vy_tst3_scale,len(vy_tst3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert all the data to np.array\n",
    "\n",
    "dem_tra_norm = np.array(dem_tra_norm)\n",
    "dem_val_norm = np.array(dem_val_norm)\n",
    "dem_tst1_norm = np.array(dem_tst1_norm)\n",
    "dem_tst2_norm = np.array(dem_tst2_norm)\n",
    "dem_tst3_norm = np.array(dem_tst3_norm)\n",
    "\n",
    "\n",
    "# wd_tra_norm = np.array(wd_tra_norm)\n",
    "# wd_val_norm = np.array(wd_val_norm)\n",
    "# wd_tst1_norm = np.array(wd_tst1_norm)\n",
    "# wd_tst2_norm = np.array(wd_tst2_norm)\n",
    "# wd_tst3_norm = np.array(wd_tst3_norm)\n",
    "\n",
    "# vx_tra_norm = np.array(vx_tra_norm)\n",
    "# vx_val_norm = np.array(vx_val_norm)\n",
    "# vx_tst1_norm = np.array(vx_tst1_norm)\n",
    "# vx_tst2_norm = np.array(vx_tst2_norm)\n",
    "# vx_tst3_norm = np.array(vx_tst3_norm)\n",
    "\n",
    "# vy_tra_norm = np.array(wd_tra_norm)\n",
    "# vy_val_norm = np.array(vy_val_norm)\n",
    "# vy_tst1_norm = np.array(vy_tst1_norm)\n",
    "# vy_tst2_norm = np.array(vy_tst2_norm)\n",
    "# vy_tst3_norm = np.array(vy_tst3_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4096, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_tra = [dem_tra_norm]\n",
    "datalist_tra = [\"dem_tra_norm\"]\n",
    "\n",
    "dataset_val = [dem_val_norm]\n",
    "datalist_val = [\"dem_val_norm\"]\n",
    "\n",
    "dataset_tst1 = [dem_tst1_norm]\n",
    "datalist_tst1 = [\"dem_tstl_norm\"]\n",
    "\n",
    "dataset_tst2 = [dem_tst2_norm]\n",
    "datalist_tst2 = [\"dem_tst2_norm\"]\n",
    "\n",
    "dataset_tst3 = [dem_tst3_norm]\n",
    "datalist_tst3 = [\"dem_tst3_norm\"]\n",
    "\n",
    "# dataset_tra = [dem_tra_norm, wd_tra_norm, vx_tra_norm, vy_tra_norm]\n",
    "# datalist_tra = [\"dem_tra_norm\",\"wd_tra_norm\", 'vx_tra_norm', 'vy_tra_norm']\n",
    "\n",
    "# dataset_val = [dem_val_norm, wd_val_norm,vx_val_norm,vy_val_norm]\n",
    "# datalist_val = [\"dem_val_norm\",\"wd_val_norm\", 'vx_val_norm', 'vy_val_norm']\n",
    "\n",
    "# dataset_tst1 = [dem_tst1_norm,wd_tst1_norm, vx_tst1_norm,vy_tst1_norm]\n",
    "# datalist_tst1 = ['dem_tst1_norm','wd_tst1_norm',  'vx_tst1_norm',  'vy_tst1_norm']\n",
    "\n",
    "# dataset_tst2 = [dem_tst2_norm, wd_tst2_norm, vx_tst2_norm, vy_tst2_norm]\n",
    "# datalist_tst2 = ['dem_tst2_norm','wd_tst2_norm',  'vx_tst2_norm',  'vy_tst2_norm']\n",
    "\n",
    "# dataset_tst3 = [dem_tst3_norm, wd_tst3_norm,vx_tst3_norm,vy_tst3_norm]\n",
    "# datalist_tst3 = ['dem_tst3_norm', 'wd_tst3_norm',  'vx_tst3_norm',  'vy_tst3_norm']\n",
    "\n",
    "display(np.shape(dem_val_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct to Folders for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_tra = proj_dir + \"\\data\\processed_data/normalized_training_data\"\n",
    "save_path_val = proj_dir + \"\\data\\processed_data/normalized_validation_data\"\n",
    "save_path_tst1 = proj_dir + \"\\data\\processed_data/normalized_test_data/test1\"\n",
    "save_path_tst2 = proj_dir + \"\\data\\processed_data/normalized_test_data/test2\"\n",
    "save_path_tst3 = proj_dir + \"\\data\\processed_data/normalized_test_data/test3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving all the normalized files to the corresponding folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = [\"DEM\"]\n",
    "# name = [\"DEm\",\"WD\", \"VX\", \"VY\"]\n",
    "fmt = '%1.4f'\n",
    "\n",
    "for i in range(len(name)):\n",
    "    os.chdir(f'{save_path_tra}/{name[i]}')\n",
    "    for j in range(len(dem_tra_norm)):\n",
    "        np.savetxt(f'{datalist_tra[i]}{j}',dataset_tra[i][0], fmt = fmt)\n",
    "        \n",
    "for i in range(len(name)):\n",
    "    os.chdir(f'{save_path_val}/{name[i]}') \n",
    "    for j in range(len(dem_val_norm)):\n",
    "        np.savetxt(f'{datalist_val[i]}{j}',dataset_val[i][0], fmt = fmt)\n",
    "    \n",
    "for i in range(len(name)):\n",
    "    os.chdir(f'{save_path_tst1}/{name[i]}')\n",
    "    for j in range(len(dem_tst1_norm)):\n",
    "        np.savetxt(f'{datalist_tst1[i]}{j}',dataset_tst1[i][0], fmt = fmt)\n",
    "        \n",
    "for i in range(len(name)):\n",
    "    os.chdir(f'{save_path_tst2}/{name[i]}')\n",
    "    for j in range(len(dem_tst2_norm)):\n",
    "        np.savetxt(f'{datalist_tst2[i]}{j}',dataset_tst2[i][0], fmt = fmt)\n",
    "        \n",
    "\n",
    "for i in range(len(name)):\n",
    "    os.chdir(f'{save_path_tst3}/{name[i]}')\n",
    "    for j in range(len(dem_tst3_norm)):\n",
    "        np.savetxt(f'{datalist_tst3[i]}{j}',dataset_tst3[i][0], fmt = fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wdtrain_dataset = TensorDataset(torch.tensor(wd_tra, dtype=torch.float32))\n",
    "# wdval_dataset = TensorDataset(torch.tensor(wd_val, dtype=torch.float32))\n",
    "# wdtest_dataset = TensorDataset(torch.tensor(wd_tst, dtype=torch.float32))\n",
    "\n",
    "# vxtrain_dataset = TensorDataset(torch.tensor(vx_tra, dtype=torch.float32))\n",
    "# vxval_dataset = TensorDataset(torch.tensor(vx_val, dtype=torch.float32))\n",
    "# vxtest_dataset = TensorDataset(torch.tensor(vx_tst, dtype=torch.float32))\n",
    "\n",
    "# vytrain_dataset = TensorDataset(torch.tensor(vy_tra, dtype=torch.float32))\n",
    "# vyval_dataset = TensorDataset(torch.tensor(vy_val, dtype=torch.float32))\n",
    "# vytest_dataset = TensorDataset(torch.tensor(vy_tst, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_pyg(graph, pos, DEM, WD, VX, VY):\n",
    "#     '''\n",
    "#     Converts a graph or mesh into a PyTorch Geometric Data type \n",
    "#     Then, add position, DEM, and water variables to data object.\n",
    "#     Adapted from https://github.com/RBTV1/SWE-GNN-paper-repository-/blob/main/database/graph_creation.py\n",
    "#     '''\n",
    "#     DEM = DEM.reshape(-1)\n",
    "\n",
    "#     edge_index = torch.LongTensor(list(graph.edges)).t().contiguous()\n",
    "#     row, col = edge_index\n",
    "\n",
    "#     data = Data()\n",
    "\n",
    "#     delta_DEM = torch.FloatTensor(DEM[col]-DEM[row])\n",
    "#     coords = torch.FloatTensor(get_coords(pos))\n",
    "#     edge_relative_distance = coords[col] - coords[row]\n",
    "#     edge_distance = torch.norm(edge_relative_distance, dim=1)\n",
    "#     edge_slope = delta_DEM/edge_distance\n",
    "\n",
    "#     data.edge_index = edge_index\n",
    "#     data.edge_distance = edge_distance\n",
    "#     data.edge_slope = edge_slope\n",
    "#     data.edge_relative_distance = edge_relative_distance\n",
    "\n",
    "#     data.num_nodes = graph.number_of_nodes()\n",
    "#     data.pos = torch.tensor(list(pos.values()))\n",
    "#     data.DEM = torch.FloatTensor(DEM)\n",
    "#     data.WD = torch.FloatTensor(WD.T)\n",
    "#     data.VX = torch.FloatTensor(VX.T)\n",
    "#     data.VY = torch.FloatTensor(VY.T)\n",
    "        \n",
    "#     return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
