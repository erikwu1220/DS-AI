{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path().resolve()\n",
    "src = cwd.parent\n",
    "root = src.parent\n",
    "sys.path.append(str(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_training_data(\n",
    "        save_path='data/normalized_data/',\n",
    "        load_path_tra_val='data/augmented_data/',\n",
    "        load_path_test='data/raw_datasets/',\n",
    "        normalize_train_validation_data=True,\n",
    "        normalize_augmented_train_validation_data=False,\n",
    "        normalize_test_data=False,\n",
    "        normalize_velocities=False,\n",
    "):\n",
    "    \n",
    "    if normalize_train_validation_data:\n",
    "\n",
    "        save_path_tra_val = save_path + 'tra_val/'\n",
    "\n",
    "        letters = ['o']\n",
    "\n",
    "        if normalize_augmented_train_validation_data:\n",
    "            letters += ['a', 'b', 'c', 'd']\n",
    "\n",
    "        for i in np.arange(1, 80+1, 1):\n",
    "            \n",
    "            for l in letters:\n",
    "\n",
    "                scales = get_scalings(i, sim_letter=l, folder=load_path_tra_val, timestep=-1, include_velocities=normalize_velocities)\n",
    "                DEM_path = scales['DEM']['path']\n",
    "                WD_path = scales['WD']['path']\n",
    "\n",
    "                with open(load_path_tra_val + DEM_path,'r') as file:\n",
    "                    data = np.loadtxt(file)\n",
    "                    data[:,-1] = (data[:,-1] - scales['DEM']['mean'])/scales['DEM']['std']\n",
    "                    np.savetxt(save_path_tra_val + DEM_path, data)\n",
    "                \n",
    "                with open(load_path_tra_val + WD_path, 'r') as file:\n",
    "                    data = np.loadtxt(file)\n",
    "                    data = (data - scales['WD']['min']) / (scales['WD']['max'] - scales['WD']['min'])\n",
    "                    np.savetxt(save_path_tra_val + WD_path, data)\n",
    "\n",
    "                if normalize_velocities:\n",
    "                    VX_path = scales['VX']['path']\n",
    "                    VY_path = scales['VY']['path']\n",
    "\n",
    "                    with open(load_path_tra_val + VX_path, 'r') as file:\n",
    "                        data = np.loadtxt(file)\n",
    "                        data = (data - scales['VX']['mean'])/scales['VX']['std']\n",
    "                        np.savetxt(save_path_tra_val + VX_path, data)\n",
    "\n",
    "                    with open(load_path_tra_val + VY_path, 'r') as file:\n",
    "                        data = np.loadtxt(file)\n",
    "                        data = (data - scales['VY']['mean'])/scales['VY']['std']\n",
    "                        np.savetxt(save_path_tra_val + VY_path, data)\n",
    "                        \n",
    "            if i//10 == i/10:            \n",
    "                print(f'Done with {i if not normalize_augmented_train_validation_data else 5*i}/{80 if not normalize_augmented_train_validation_data else 400} training/validation datasets')\n",
    "\n",
    "    if normalize_test_data:\n",
    "        for n in [1, 2, 3]:\n",
    "            save_path_test = save_path + f'test{n}/'\n",
    "            load_path_test_current = load_path_test + f'test{n}/'\n",
    "\n",
    "            if n == 1:\n",
    "                sim_numbers = np.arange(500, 519+1, 1)\n",
    "            elif n == 2:\n",
    "                sim_numbers = np.arange(10000, 10020+1, 1)\n",
    "            elif n == 3:\n",
    "                sim_numbers = np.arange(15001, 15010+1, 1)\n",
    "\n",
    "            for i in sim_numbers:\n",
    "                \n",
    "                scales = get_scalings(i, sim_letter='', folder=load_path_test_current, timestep=-1, include_velocities=normalize_velocities)\n",
    "                DEM_path = scales['DEM']['path']\n",
    "                WD_path = scales['WD']['path']\n",
    "\n",
    "                with open(load_path_test_current + DEM_path,'r') as file:\n",
    "                    data = np.loadtxt(file)\n",
    "                    data[:,-1] = (data[:,-1] - scales['DEM']['mean'])/scales['DEM']['std']\n",
    "                    np.savetxt(save_path_test + DEM_path, data)\n",
    "                \n",
    "                with open(load_path_test_current + WD_path, 'r') as file:\n",
    "                    data = np.loadtxt(file)\n",
    "                    data = (data - scales['WD']['min']) / (scales['WD']['max'] - scales['WD']['min'])\n",
    "                    np.savetxt(save_path_test + WD_path, data)\n",
    "\n",
    "                if normalize_velocities:\n",
    "                    VX_path = scales['VX']['path']\n",
    "                    VY_path = scales['VY']['path']\n",
    "\n",
    "                    with open(load_path_test_current + VX_path, 'r') as file:\n",
    "                        data = np.loadtxt(file)\n",
    "                        data = (data - scales['VX']['mean'])/scales['VX']['std']\n",
    "                        np.savetxt(save_path_test + VX_path, data)\n",
    "\n",
    "                    with open(load_path_test_current + VY_path, 'r') as file:\n",
    "                        data = np.loadtxt(file)\n",
    "                        data = (data - scales['VY']['mean'])/scales['VY']['std']\n",
    "                        np.savetxt(save_path_test + VY_path, data)\n",
    "            \n",
    "            print(f'Done with test set {n}')\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def get_scalings(sim_number, \n",
    "                 sim_letter='o',\n",
    "                 folder='data/augmented_data/',\n",
    "                 timestep=-1,\n",
    "                 include_velocities=False):\n",
    "    \"\"\"Gets mean and variance for a given simulation. These are calculated for the DEM, VX, VY, WD. \n",
    "    The statistics for the DEM are constant throughout a simulation.\n",
    "    The statistics for the VX, VY, WD are calculated from the LAST timestep of a simulation.\n",
    "    \"folder\" refers to the folder containing the DEM, VX, VY, WD folders.\n",
    "    \"timestep\" refers to the timestep to use when calculating the normalization parameters for VX, VY, WD.\n",
    "    \"include_velocities\": set to True in order to also calculate statistics for velocities.\n",
    "    \"\"\"\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    if len(sim_letter)>0:\n",
    "        sim_letter=f'_{sim_letter}'\n",
    "\n",
    "    DEM_path = f'DEM/DEM_{str(int(sim_number))}{str(sim_letter)}.txt'\n",
    "    WD_path = f'WD/WD_{str(int(sim_number))}{str(sim_letter)}.txt'\n",
    "\n",
    "    with open(folder + DEM_path, 'r') as file:\n",
    "        data = np.loadtxt(file)[:,-1]\n",
    "\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        minimum = np.min(data)\n",
    "        maximum = np.max(data)\n",
    "\n",
    "        stats['DEM'] = {'path':DEM_path,\n",
    "                        'mean':mean,\n",
    "                        'std':std,\n",
    "                        'min':minimum,\n",
    "                        'max':maximum}\n",
    "        \n",
    "    with open(folder + WD_path, 'r') as file:\n",
    "        data = np.loadtxt(file)[timestep,:]\n",
    "        # print(data)\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        minimum = np.min(data)\n",
    "        maximum = np.max(data)\n",
    "\n",
    "        stats['WD'] = {'path':WD_path,\n",
    "                       'mean':mean,\n",
    "                       'std':std,\n",
    "                       'min':minimum,\n",
    "                       'max':maximum}\n",
    "\n",
    "    if include_velocities:\n",
    "        VX_path = f'VX/VX_{str(int(sim_number))}_{str(sim_letter)}.txt'\n",
    "        VY_path = f'VY/VY_{str(int(sim_number))}_{str(sim_letter)}.txt'\n",
    "\n",
    "        with open(folder + VX_path, 'r') as file:\n",
    "            data = np.loadtxt(file)[timestep,:]\n",
    "\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            minimum = np.min(data)\n",
    "            maximum = np.max(data)\n",
    "\n",
    "            stats['VX'] = {'path':VX_path,\n",
    "                           'mean':mean,\n",
    "                           'std':std,\n",
    "                           'min':minimum,\n",
    "                           'max':maximum}\n",
    "            \n",
    "        with open(folder + VY_path, 'r') as file:\n",
    "            data = np.loadtxt(file)[timestep,:]\n",
    "\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            minimum = np.min(data)\n",
    "            maximum = np.max(data)\n",
    "\n",
    "            stats['VY'] = {'path':VY_path,\n",
    "                           'mean':mean,\n",
    "                           'std':std,\n",
    "                           'min':minimum,\n",
    "                           'max':maximum}\n",
    "    # print(stats)\n",
    "    return stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khdeb\\AppData\\Local\\Temp\\ipykernel_11160\\986236242.py:35: RuntimeWarning: divide by zero encountered in divide\n",
      "  data = (data - scales['WD']['min']) / (scales['WD']['max'] - scales['WD']['min'])\n",
      "C:\\Users\\khdeb\\AppData\\Local\\Temp\\ipykernel_11160\\986236242.py:35: RuntimeWarning: invalid value encountered in divide\n",
      "  data = (data - scales['WD']['min']) / (scales['WD']['max'] - scales['WD']['min'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 50/400 training/validation datasets\n",
      "Done with 100/400 training/validation datasets\n",
      "Done with 150/400 training/validation datasets\n",
      "Done with 200/400 training/validation datasets\n",
      "Done with 250/400 training/validation datasets\n",
      "Done with 300/400 training/validation datasets\n",
      "Done with 350/400 training/validation datasets\n",
      "Done with 400/400 training/validation datasets\n",
      "Done with test set 1\n",
      "Done with test set 2\n",
      "Done with test set 3\n"
     ]
    }
   ],
   "source": [
    "save_path_tra = str(root) + \"/data/normalized_data/\"\n",
    "load_path_tra_val = str(root) + \"/data/augmented_data/\"\n",
    "load_path_test = str(root) + \"/data/raw_datasets/\"\n",
    "\n",
    "normalize_training_data(\n",
    "        save_path=save_path_tra,\n",
    "        load_path_tra_val=load_path_tra_val,\n",
    "        load_path_test=load_path_test,\n",
    "        normalize_train_validation_data=True,\n",
    "        normalize_augmented_train_validation_data=True,\n",
    "        normalize_test_data=True,\n",
    "        normalize_velocities=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10547118184197812\n"
     ]
    }
   ],
   "source": [
    "file = str(root) + \"/data/raw_datasets/test1/WD/WD_500.txt\"\n",
    "\n",
    "wd_500 = np.loadtxt(file)\n",
    "\n",
    "print(np.mean(wd_500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BELOW ONLY JUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khdeb\\Projects\\DS-AI/data/augmented_data/DEM/DEM_1_o.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "folder_path = str(root) + \"/data/augmented_data/\"\n",
    "DEM_path = folder_path + \"DEM/DEM_1_o.txt\"\n",
    "print(DEM_path)\n",
    "\n",
    "with open(DEM_path, 'r') as DEM_file:\n",
    "    DEM = np.loadtxt(DEM_file)[:,-1]\n",
    "    # mean = np.mean()\n",
    "    # std = np.std()\n",
    "    \n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khdeb\\Projects\\DS-AI/data/augmented_data/WD/WD_1_o.txt\n",
      "[0.     0.     0.     ... 0.     1.0697 0.    ]\n",
      "(4096,)\n"
     ]
    }
   ],
   "source": [
    "folder_path = str(root) + \"/data/augmented_data/\"\n",
    "WD_path = folder_path + \"WD/WD_1_o.txt\"\n",
    "print(WD_path)\n",
    "\n",
    "with open(WD_path, 'r') as file:\n",
    "    data = np.loadtxt(file)[:, -1]\n",
    "    print(data)\n",
    "    print(data.shape)\n",
    "    # mean = np.mean()\n",
    "    # std = np.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1445413425418827\n"
     ]
    }
   ],
   "source": [
    "scalers = get_scalings(1, \n",
    "                 sim_letter='o',\n",
    "                 folder=folder_path,\n",
    "                 timestep=-1,\n",
    "                 include_velocities=False)\n",
    "\n",
    "print(scalers['DEM']['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "var = [1]\n",
    "var += [1, 2, 3]\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(11//10==11/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
