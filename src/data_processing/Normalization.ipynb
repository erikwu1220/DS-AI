{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os.path\n",
    "import glob\n",
    "from urllib.request import urlretrieve\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "# Imported because Roberto also did it.\n",
    "# from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagi8\\Documents\\GitHub\\DS-AI\\data\\processed_data\\normalized_test_data\\test3\\data/raw_datasets\n",
      "C:\\Users\\sagi8\\Documents\\GitHub\\DS-AI\\data\\processed_data\\normalized_test_data\\test3\\data/augmented_data\n"
     ]
    }
   ],
   "source": [
    "path = pathlib.Path().resolve()\n",
    "proj_dir = str(path.parent)\n",
    "path_raw = proj_dir + \"\\data/raw_datasets\"\n",
    "path_aug = proj_dir + \"\\data/augmented_data\"\n",
    "print(path_raw)\n",
    "print(path_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Reading me!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the augmented validation and training data. (from Patrick)\\\n",
    "Load the original testing data (Raw dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_variable = ['/DEM',\"/WD\",\"/VX\",\"/VY\"]\n",
    "test = ['/test1','/test2','/test3']\n",
    "\n",
    "dem_tra_val = []\n",
    "dem_tst1 = []\n",
    "dem_tst2 = []\n",
    "dem_tst3 = []\n",
    "\n",
    "wd_tra_val = []\n",
    "wd_tst1 = []\n",
    "wd_tst2 = []\n",
    "wd_tst3 = []\n",
    "\n",
    "vx_tra_val = []\n",
    "vx_tst1 = []\n",
    "vx_tst2 = []\n",
    "vx_tst3 = []\n",
    "\n",
    "vy_tra_val = []\n",
    "vy_tst1 = []\n",
    "vy_tst2 = []\n",
    "vy_tst3 = []\n",
    "\n",
    "var_list = [[dem_tra_val,dem_tst1,dem_tst2,dem_tst3],\n",
    "            [wd_tra_val,wd_tst1,wd_tst2,wd_tst3],\n",
    "            [vx_tra_val,vx_tst1,vx_tst2,vx_tst3],\n",
    "            [vy_tra_val,vy_tst1,vy_tst2,vy_tst3]]\n",
    "\n",
    "for i in range(len(loading_variable)):\n",
    "    tv = glob.glob(path_aug + loading_variable[i] + '\\*.txt')\n",
    "    t1 = glob.glob(path_raw + test[0] + loading_variable[i] + '\\*.txt')\n",
    "    t2 = glob.glob(path_raw + test[1] + loading_variable[i] + '\\*.txt')\n",
    "    t3 = glob.glob(path_raw + test[2] + loading_variable[i] + '\\*.txt')\n",
    "\n",
    "    tem_var = var_list[i]\n",
    "\n",
    "    for j in range(len(tv)):\n",
    "        tem_var[0].append(np.loadtxt(tv[j]))\n",
    "\n",
    "    for j in range(len(t1)):\n",
    "        tem_var[1].append(np.loadtxt(t1[j]))\n",
    "\n",
    "    for j in range(len(t2)):\n",
    "        tem_var[2].append(np.loadtxt(t2[j]))\n",
    "\n",
    "    for j in range(len(t3)):\n",
    "        tem_var[3].append(np.loadtxt(t3[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 4096, 3)\n",
      "(20, 4096, 3)\n",
      "(21, 4096, 3)\n",
      "(10, 16384, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(dem_tra_val))\n",
    "print(np.shape(dem_tst1))\n",
    "print(np.shape(dem_tst2))\n",
    "print(np.shape(dem_tst3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the train and validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vx_tra): 280\n",
      "len(vx_val): 120\n",
      "len(vx_tst1): 20\n",
      "len(vx_tst2): 21\n",
      "len(vx_tst3): 10\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset (in total 80) into following portion: training(70%) and validation(30%)\n",
    "# We already have the testing dataset\n",
    "# DEM\n",
    "dem_tra, dem_val, wd_tra, wd_val, vx_tra, vx_val, vy_tra, vy_val, idem_tra, idem_val = train_test_split(\n",
    "    dem_tra_val, wd_tra_val, vx_tra_val, vy_tra_val, np.arange(len(dem_tra_val)), train_size=0.7, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"len(vx_tra): {len(dem_tra)}\")\n",
    "print(f\"len(vx_val): {len(dem_val)}\")\n",
    "print(f\"len(vx_tst1): {len(dem_tst1)}\")\n",
    "print(f\"len(vx_tst2): {len(dem_tst2)}\")\n",
    "print(f\"len(vx_tst3): {len(dem_tst3)}\")\n",
    "\n",
    "# Note that the form of the all the training, testing and validating dataset are \"a list containing multiple arrays\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here use the Min-max normalization to normalize the water depth over the entire simulation sequence (0-130)\n",
    "def scale_sequences(X,scaler=None,scaler_type='minmax'):\n",
    "    \"\"\"\n",
    "    Uses a minmax scaler to transform sequences. The scaler is created if no scaler is passed as argument.\n",
    "    Adapted from exercise notebook on drinking water demand.\n",
    "\n",
    "    The input parameter X is a two-dimensional array.\n",
    "    \"\"\"\n",
    "\n",
    "    Xshape=X.shape\n",
    "    if scaler:\n",
    "        X = scaler.transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X\n",
    "    else:\n",
    "        if scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise Exception(\"Type of scikit-learn scaler not supported. Choose 'standard' or 'minmax.\")\n",
    "        X = scaler.fit_transform(X.reshape(-1,1)).reshape(Xshape)\n",
    "        return X, scaler\n",
    "\n",
    "def denormalize(image_tensor, mean, std):\n",
    "    # Denormalize the image\n",
    "    denorm_img = image_tensor * std[:, None, None] + mean[:, None, None]\n",
    "    # Clip values to be between 0 and 1\n",
    "    denorm_img = denorm_img.clip(0, 1)\n",
    "    return denorm_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_arr = [dem_tra[0], wd_tra[0], vx_tra[0], vy_tra[0]]\n",
    "val_arr = [dem_val[0], wd_val[0], vx_val[0], vy_val[0]]\n",
    "tst1_arr = [dem_tst1[0], wd_tst1[0], vx_tst1[0], vy_tst1[0]]\n",
    "tst2_arr = [dem_tst2[0], wd_tst2[0], vx_tst2[0], vy_tst2[0]]\n",
    "tst3_arr = [dem_tst3[0], wd_tst3[0], vx_tst3[0], vy_tst3[0]]\n",
    "\n",
    "splitlis = [[dem_tra, dem_val, dem_tst1, dem_tst2, dem_tst3],\n",
    "             [wd_tra, wd_val, wd_tst1, wd_tst2, wd_tst3],\n",
    "             [vx_tra, vx_val, vx_tst1, vx_tst2, vx_tst3],\n",
    "             [vy_tra, vy_val, vy_tst1, vy_tst2, vy_tst3]]\n",
    "\n",
    "scalelist = [[dem_tra_scale, dem_val_scale, dem_tst1_scale, dem_tst2_scale, dem_tst3_scale],\n",
    "             [wd_tra_scale, wd_val_scale, wd_tst1_scale, wd_tst2_scale, wd_tst3_scale],\n",
    "             [vx_tra_scale, vx_val_scale, vx_tst1_scale, vx_tst2_scale, vx_tst3_scale],\n",
    "             [vy_tra_scale, vy_val_scale, vy_tst1_scale, vy_tst2_scale, vy_tst3_scale]]\n",
    "\n",
    "\n",
    "datalist_tra = [\"dem_tra_norm\",\"wd_tra_norm\", 'vx_tra_norm', 'vy_tra_norm']\n",
    "\n",
    "datalist_val = [\"dem_val_norm\",\"wd_val_norm\", 'vx_val_norm', 'vy_val_norm']\n",
    "\n",
    "datalist_tst1 = ['dem_tst1_norm','wd_tst1_norm', 'vx_tst1_norm', 'vy_tst1_norm']\n",
    "\n",
    "datalist_tst2 = ['dem_tst2_norm','wd_tst2_norm', 'vx_tst2_norm', 'vy_tst2_norm']\n",
    "\n",
    "datalist_tst3 = ['dem_tst3_norm', 'wd_tst3_norm', 'vx_tst3_norm', 'vy_tst3_norm']\n",
    "\n",
    "# Direct to Folders for saving\n",
    "save_path_tra = proj_dir + \"\\data\\processed_data/normalized_training_data\"\n",
    "save_path_val = proj_dir + \"\\data\\processed_data/normalized_validation_data\"\n",
    "save_path_tst1 = proj_dir + \"\\data\\processed_data/normalized_test_data/test1\"\n",
    "save_path_tst2 = proj_dir + \"\\data\\processed_data/normalized_test_data/test2\"\n",
    "save_path_tst3 = proj_dir + \"\\data\\processed_data/normalized_test_data/test3\"\n",
    "\n",
    "name = [\"DEM\",\"WD\", \"VX\", \"VY\"]\n",
    "fmt = '%1.4f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(splitlis[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  range(len(tra_arr)):\n",
    "    tra_con = tra_arr[i]\n",
    "    val_con = val_arr[i]\n",
    "    tst1_con = tst1_arr[i]\n",
    "    tst2_con = tst2_arr[i]\n",
    "    tst3_con = tst3_arr[i]\n",
    "\n",
    "    # training\n",
    "    for j in range(1,len(dem_tra)):\n",
    "        tra_con = np.concatenate((tra_con, splitlis[i][0][j]),axis=0)\n",
    "\n",
    "    # validation\n",
    "    for j in range(1,len(dem_val)):\n",
    "        val_con = np.concatenate((val_con, splitlis[i][1][j]),axis=0)\n",
    "\n",
    "    # test 1\n",
    "    for j in range(1,len(dem_tst1)):\n",
    "        tst1_con = np.concatenate((tst1_con, splitlis[i][2][j]),axis=0)\n",
    "\n",
    "    #test 2\n",
    "    for j in range(1,len(dem_tst2)):\n",
    "        tst2_con = np.concatenate((tst2_con, splitlis[i][3][j]),axis=0)\n",
    "\n",
    "    #test 3\n",
    "    for j in range(1,len(dem_tst3)):\n",
    "        tst3_con = np.concatenate((tst3_con, splitlis[i][4][j]),axis=0)\n",
    "\n",
    "    tra_scale, scaler = scale_sequences(tra_con ,scaler=None,scaler_type='minmax')\n",
    "    val_scale, scaler = scale_sequences(val_con ,scaler=None,scaler_type='minmax')\n",
    "    tst1_scale, scaler = scale_sequences(tst1_con ,scaler=None,scaler_type='minmax')\n",
    "    tst2_scale, scaler = scale_sequences(tst2_con ,scaler=None,scaler_type='minmax')\n",
    "    tst3_scale, scaler = scale_sequences(tst3_con ,scaler=None,scaler_type='minmax')\n",
    "\n",
    "    tra_norm = np.vsplit(tra_scale,len(dem_tra))\n",
    "    val_norm = np.vsplit(val_scale,len(dem_val))\n",
    "    tst1_norm = np.vsplit(tst1_scale,len(dem_tst1))\n",
    "    tst2_norm = np.vsplit(tst2_scale,len(dem_tst2))\n",
    "    tst3_norm = np.vsplit(tst3_scale,len(dem_tst3))\n",
    "\n",
    "\n",
    "    os.chdir(f'{save_path_tra}/{name[i]}')\n",
    "    for j in range(len(tra_norm)):\n",
    "        np.savetxt(f'{datalist_tra[i]}{j}',tra_norm[j], fmt = fmt)\n",
    "\n",
    "\n",
    "    os.chdir(f'{save_path_val}/{name[i]}')\n",
    "    for j in range(len(val_norm)):\n",
    "        np.savetxt(f'{datalist_val[i]}{j}',val_norm[j], fmt = fmt)\n",
    "\n",
    "\n",
    "    os.chdir(f'{save_path_tst1}/{name[i]}')\n",
    "    for j in range(len(tst1_norm)):\n",
    "        np.savetxt(f'{datalist_tst1[i]}{j}',tst1_norm[j], fmt = fmt)\n",
    "\n",
    "\n",
    "    os.chdir(f'{save_path_tst2}/{name[i]}')\n",
    "    for j in range(len(tst2_norm)):\n",
    "        np.savetxt(f'{datalist_tst2[i]}{j}',tst2_norm[j], fmt = fmt)\n",
    "\n",
    "\n",
    "    os.chdir(f'{save_path_tst3}/{name[i]}')\n",
    "    for j in range(len(tst3_norm)):\n",
    "        np.savetxt(f'{datalist_tst3[i]}{j}',tst3_norm[j], fmt = fmt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
