{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ef1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import os.path\n",
    "import glob\n",
    "import pathlib\n",
    "\n",
    "from cycler import cycler\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the color scheme\n",
    "sns.set_theme()\n",
    "colors = ['#0076C2', '#EC6842', '#A50034', '#009B77', '#FFB81C', '#E03C31', '#6CC24A', '#EF60A3', '#0C2340', '#00B8C8', '#6F1D77']\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=colors)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90c9dd",
   "metadata": {},
   "source": [
    "# Model (Copy from FAT workshop)\n",
    "\n",
    "Your main task throughout this notebook will be to define a UNET-based CNN architecture.\n",
    "You are free to design the architecture as you prefer, as long as the model does its purpose and produces decent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7f71aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create you own CNN model\n",
    "\n",
    "# Define the model\n",
    "# model = ...\n",
    "\n",
    "# ---------------------- student exercise --------------------------------- #\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "        layers.append(nn.PReLU())\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias))\n",
    "                \n",
    "        self.cnnblock = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnnblock(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[32, 64, 128], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias, \n",
    "                     batch_norm=batch_norm) \n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            outs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return outs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[128, 64, 32], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[block], channels[block+1], kernel_size=2, padding=0, stride=2) \n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.dec_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias, \n",
    "                     batch_norm=batch_norm)\n",
    "             for block in range(len(channels)-1)]\n",
    "             )\n",
    "        \n",
    "    def forward(self, x, x_skips):\n",
    "        for i in range(len(x_skips)):\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat((x, x_skips[-(1+i)]), dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "\n",
    "        x = self.dec_blocks[-1](x)\n",
    "        return x\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, node_features, out_dim=1, n_downsamples=3, initial_hid_dim=64, batch_norm=True, \n",
    "                 bias=True):\n",
    "        super(CNN, self).__init__()\n",
    "        hidden_channels = [initial_hid_dim*2**i for i in range(n_downsamples)]\n",
    "        encoder_channels = [node_features]+hidden_channels\n",
    "        decoder_channels = list(reversed(hidden_channels))+[out_dim]\n",
    "\n",
    "        self.encoder = Encoder(encoder_channels, kernel_size=3, padding=1, \n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "        self.decoder = Decoder(decoder_channels, kernel_size=3, padding=1, \n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x[-1], x[:-1])\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x\n",
    "# ---------------------- student exercise --------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "310c5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = 3 # WD, VX, VY\n",
    "model = CNN(node_features=node_features, n_downsamples=4, initial_hid_dim=32, \n",
    "            batch_norm=True, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70e762",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "Loading the normalized data from the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06d6695b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 849. MiB for an array with shape (280, 4096, 97) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2033\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2033\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m [wd_val\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mloadtxt(wd_vald[i])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(wd_vald))]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Check if we load the data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwd_tra\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(wd_tra))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(wd_val))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaie\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2035\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   2033\u001b[0m     result \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   2034\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m-> 2035\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   2036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 849. MiB for an array with shape (280, 4096, 97) and data type float64"
     ]
    }
   ],
   "source": [
    "path = pathlib.Path().resolve()\n",
    "proj_dir = str(path.parent)\n",
    "\n",
    "path_tra = proj_dir + \"\\data/processed_data/normalized_training_data\"\n",
    "path_val = proj_dir + \"\\data/processed_data/normalized_validation_data\"\n",
    "path_tst = proj_dir + \"\\data/processed_data/normalized_test_data\"\n",
    "\n",
    "# Load training data\n",
    "wd_train = glob.glob(path_tra + '/WD/*')\n",
    "wd_tra = []\n",
    "[wd_tra.append(np.loadtxt(wd_train[i])) for i in range(len(wd_train))]\n",
    "\n",
    "# Load Validation data\n",
    "wd_vald = glob.glob(path_val + '/WD/*')\n",
    "wd_val = []\n",
    "[wd_val.append(np.loadtxt(wd_vald[i])) for i in range(len(wd_vald))]\n",
    "    \n",
    "\n",
    "# Check if we load the data\n",
    "print(np.shape(wd_tra))\n",
    "print(type(wd_tra))\n",
    "print(np.shape(wd_val))\n",
    "print(type(wd_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83203331",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Define the training and evaluation functions needed to update the model's parameters.\n",
    "\n",
    "Tip: you can use the same training and evaluation function we have used so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f14797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train() # specifies that the model is in training mode\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "\n",
    "        # Model prediction\n",
    "        preds = model(x)\n",
    "        \n",
    "        # MSE loss function\n",
    "        loss = nn.MSELoss()(preds, y)\n",
    "        \n",
    "        losses.append(loss.cpu().detach())\n",
    "        \n",
    "        # Backpropagate and update weights\n",
    "        loss.backward()   # compute the gradients using backpropagation\n",
    "        optimizer.step()  # update the weights with the optimizer\n",
    "        optimizer.zero_grad(set_to_none=True)   # reset the computed gradients\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18d26d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, loader, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval() # specifies that the model is in evaluation mode\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            y = batch[1]\n",
    "\n",
    "            # Model prediction\n",
    "            preds = model(x)\n",
    "\n",
    "            # MSE loss function\n",
    "            loss = nn.MSELoss()(preds, y)\n",
    "            losses.append(loss.cpu().detach())\n",
    "\n",
    "    losses = np.array(losses).mean()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ca176",
   "metadata": {},
   "source": [
    "### Define the training paramters, the optimizer, and the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f47c88b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wd_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create the training and validation dataloaders to \"feed\" data to the model in batches\u001b[39;00m\n\u001b[0;32m     10\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(wd_tra, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mwd_val\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# test_loader = DataLoader(normalized_test_dataset, batch_size=batch_size, shuffle=False)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wd_val' is not defined"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(wd_tra, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(wd_val, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(normalized_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f4aec",
   "metadata": {},
   "source": [
    "## Training and validating\n",
    "\n",
    "And finally, it's time to train you model, with many epochs.\n",
    "\n",
    "Make sure to train for enough epochs not to end training preemptively.\n",
    "\n",
    "Remember to save your training and validation losses to check if your model is training properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- student exercise --------------------------------- #\n",
    "#create vectors for the training and validation loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    # Model training\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device=device)\n",
    "\n",
    "    # Model validation\n",
    "    val_loss = evaluation(model, val_loader, device=device)\n",
    "\n",
    "    if epoch == 1:\n",
    "        best_loss = val_loss\n",
    "    \n",
    "    if val_loss<=best_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        print(\"epoch:\",epoch, \"\\t training loss:\", np.round(train_loss,4),\n",
    "                            \"\\t validation loss:\", np.round(val_loss,4))\n",
    "        \n",
    "model = copy.deepcopy(best_model)\n",
    "# ---------------------- student exercise --------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluation(model, test_loader, device=device)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbae6a",
   "metadata": {},
   "source": [
    "## Losses\n",
    "\n",
    "Let's check if your training and validation losses are decreasing with the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.yscale('log')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c474fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd10a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
