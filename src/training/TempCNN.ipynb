{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path().resolve()\n",
    "src = cwd.parent\n",
    "root = src.parent\n",
    "sys.path.append(str(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.RNN import SimpleRNN\n",
    "from utils.simulation import Simulation\n",
    "from utils.train import train_and_validate, evaluate_model, train\n",
    "from utils.watertopo import WaterTopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA enabled? False\n",
      "Number of GPUs 0\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())\n",
    "print(\"Number of GPUs\",torch.cuda.device_count())\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "tra_datapath = str(root)+\"/data/normalized_data/tra_val\"\n",
    "test_datapath = str(root)+\"/data/normalized_data/test1\"\n",
    "grid_size = 64\n",
    "sim_amount = 100\n",
    "test_amount = 20\n",
    "\n",
    "# select one simulation for training\n",
    "sims = Simulation.load_simulations(save_folder = tra_datapath,\n",
    "                                  sim_amount = sim_amount,\n",
    "                                  number_grids = grid_size)\n",
    "\n",
    "test = Simulation.load_simulations(save_folder=test_datapath,\n",
    "                                  sim_amount = test_amount,\n",
    "                                  number_grids = grid_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1,3, figsize = (12,10))\n",
    "# axs[0].imshow(test[0].topography, cmap = 'terrain')\n",
    "# axs[1].imshow(test[0].wd[-1], cmap = 'Blues')\n",
    "# axs[2].quiver(np.arange(0, 64), np.arange(0, 64), test[0].vx[-1], test[0].vy[-1])\n",
    "# axs[2].set_aspect('equal', adjustable='box')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the simulations for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocessing(data, sim_amount, timestep, gridsize):\n",
    "    '''\n",
    "    # Reshape the data into into the 2D array with\n",
    "    # each row representing the pixels (each pixel is regarded as a new variable)\n",
    "    # each column representing the timestep (condider the temporal features as the other dimension of images)\n",
    "    '''\n",
    "    dem = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "    wd = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "    wl = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "    vx = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "    vy = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "\n",
    "    for i in range(sim_amount):\n",
    "        wd[i] =  data[i].wd\n",
    "        dem[i] = data[i].topography\n",
    "        vx[i] = data[i].vx\n",
    "        vy[i] = data[i].vy\n",
    "        wl[i] = wd[i] + dem[i]\n",
    "\n",
    "    def dataconvert(meshdata,timestep,gridsize):\n",
    "        tempCNN_array = np.zeros((gridsize*gridsize,timestep))\n",
    "        for i in range(timestep):\n",
    "            tempCNN_array[:,i] = meshdata[i].reshape(-1)\n",
    "        return tempCNN_array\n",
    "\n",
    "    dem_new = []\n",
    "    wd_new = []\n",
    "    wl_new = []\n",
    "    vx_new = []\n",
    "    vy_new = []\n",
    "\n",
    "    for i in range(sim_amount):\n",
    "        dem_new.append(dataconvert(dem[i],timestep,gridsize))\n",
    "        wd_new.append(dataconvert(wd[i],timestep,gridsize))\n",
    "        wl_new.append(dataconvert(wl[i],timestep,gridsize))\n",
    "        vx_new.append(dataconvert(vx[i],timestep,gridsize))\n",
    "        vy_new.append(dataconvert(vy[i],timestep,gridsize))\n",
    "\n",
    "    return dem_new, wd_new, wl_new, vx_new, vy_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = len(sims[0].wd)\n",
    "\n",
    "# Training data\n",
    "dem_new, wd_new, wl_new, vx_new, vy_new = dataprocessing(sims, sim_amount = sim_amount, timestep = timestep, gridsize= grid_size)\n",
    "\n",
    "# Testing data\n",
    "dem_test, wd_test, wl_test, vx_test, vy_test = dataprocessing(test, sim_amount = test_amount, timestep = timestep, gridsize= grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# timestep = len(sims[0].wd)\n",
    "# wd = np.zeros((sim_amount,timestep,grid_size,grid_size))\n",
    "# dem = np.zeros((sim_amount,timestep,grid_size,grid_size))\n",
    "# # create waterlavel as the output\n",
    "# wl = np.zeros((sim_amount,timestep,grid_size,grid_size))\n",
    "\n",
    "# for i in range(sim_amount):\n",
    "#     wd[i] =  sims[i].wd\n",
    "#     dem[i] = sims[i].topography\n",
    "#     wl[i] = wd[i] + dem[i]\n",
    "\n",
    "\n",
    "# print(dem.shape)\n",
    "# print(wd.shape)\n",
    "\n",
    "# def dataconvert(meshdata,timestep,gridsize):\n",
    "#     tempCNN_array = np.zeros((gridsize*gridsize,timestep))\n",
    "#     for i in range(timestep):\n",
    "#         tempCNN_array[:,i] = meshdata[i].reshape(-1)\n",
    "#     return tempCNN_array\n",
    "\n",
    "# dem_new = []\n",
    "# wd_new = []\n",
    "# wl_new = []\n",
    "\n",
    "# for i in range(sim_amount):\n",
    "#     dem_new.append(dataconvert(dem[i],timestep,grid_size))\n",
    "#     wd_new.append(dataconvert(wd[i],timestep,grid_size))\n",
    "#     wl_new.append(dataconvert(wl[i],timestep,grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the color\n",
    "# color_dem = cm.gist_earth\n",
    "# color_water = 'Blues'\n",
    "\n",
    "# fig, axs = plt.subplots(2,5 , figsize = (12,10))\n",
    "# axs[0,0].imshow(dem_new[0], cmap = color_dem)\n",
    "# axs[0,1].imshow(wd_new[0], cmap = color_water)\n",
    "# axs[0,2].imshow(wl_new[0], cmap = color_dem)\n",
    "# axs[0,3].imshow(vx_new[0])\n",
    "# axs[0,4].imshow(vy_new[0])\n",
    "\n",
    "# axs[1,0].imshow(dem_test[0], cmap = color_dem)\n",
    "# axs[1,1].imshow(wd_test[0], cmap = color_water)\n",
    "# axs[1,2].imshow(wl_test[0], cmap = color_dem)\n",
    "# axs[1,3].imshow(vx_new[0])\n",
    "# axs[1,4].imshow(vy_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tra, dem_val, wd_tra, wd_val, wl_tra, wl_val, vx_tra, vx_val, vy_tra, vy_val, ix_tra, ix_tst = train_test_split(\n",
    "    dem_new, wd_new, wl_new, vx_new, vy_new, np.arange(sim_amount), test_size=0.30, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input for the model by combining dem and water depth and the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 4, 4096, 97)\n",
      "(70, 1, 4096, 97)\n",
      "(30, 4, 4096, 97)\n",
      "(30, 1, 4096, 97)\n",
      "(20, 4, 4096, 97)\n",
      "(20, 1, 4096, 97)\n"
     ]
    }
   ],
   "source": [
    "train_inputs = np.stack((dem_tra, wd_tra, vx_tra, vy_tra),1)\n",
    "train_outputs = np.array(wl_tra)[:,None]\n",
    "\n",
    "val_inputs = np.stack((dem_val, wd_val, vx_val, vy_val),1)\n",
    "val_outputs = np.array(wl_val)[:,None]\n",
    "\n",
    "test_inputs = np.stack((dem_test, wd_test, vx_test, vy_test),1)\n",
    "test_outputs = np.array(wl_test)[:,None]\n",
    "\n",
    "print(train_inputs.shape)\n",
    "print(train_outputs.shape)\n",
    "\n",
    "print(val_inputs.shape)\n",
    "print(val_outputs.shape)\n",
    "\n",
    "print(test_inputs.shape)\n",
    "print(test_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.float32), torch.tensor(train_outputs, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(val_inputs, dtype=torch.float32), torch.tensor(val_outputs, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(test_inputs, dtype=torch.float32), torch.tensor(test_outputs, dtype=torch.float32 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Unet Encoder-Decoder CNN to train the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "        layers.append(nn.PReLU())\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias))\n",
    "\n",
    "        self.cnnblock = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnnblock(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[4, 8, 16], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            outs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return outs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[16, 8, 4], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[block], channels[block+1], kernel_size=2, padding=0, stride=1)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.dec_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "             for block in range(len(channels)-1)]\n",
    "             )\n",
    "\n",
    "    def forward(self, x, x_skips):\n",
    "        for i in range(len(x_skips)):\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat((x, x_skips[-(1+i)]), dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "\n",
    "        x = self.dec_blocks[-1](x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, node_features, out_dim=1, n_downsamples=3, initial_hid_dim=64, batch_norm=True,\n",
    "                 bias=True):\n",
    "        super(CNN, self).__init__()\n",
    "        hidden_channels = [initial_hid_dim*2**i for i in range(n_downsamples)]\n",
    "        encoder_channels = [node_features]+hidden_channels\n",
    "        decoder_channels = list(reversed(hidden_channels))+[out_dim]\n",
    "\n",
    "        self.encoder = Encoder(encoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "        self.decoder = Decoder(decoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x[-1], x[:-1])\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = 4 # Water Depth and DEM\n",
    "model = CNN(node_features=node_features,\n",
    "            n_downsamples=3,\n",
    "            initial_hid_dim=4,\n",
    "            batch_norm=True, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training parameter, the optimizer, and the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "num_epochs = 350\n",
    "device = 'cpu'\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "model_name = 'TempCNN(VX,VY)'\n",
    "save_path = \"../results/trained_models/\" + model_name\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader.dataset[0]\n",
    "# val_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/350 Train Loss: 1.1051, Validation Loss: 1.1179 Best Validation Loss: 1.1179\n",
      "Epoch 10/350 Train Loss: 1.0805, Validation Loss: 1.0821 Best Validation Loss: 1.0821\n",
      "Epoch 15/350 Train Loss: 1.0457, Validation Loss: 1.0457 Best Validation Loss: 1.0457\n",
      "Epoch 20/350 Train Loss: 1.0222, Validation Loss: 1.0169 Best Validation Loss: 1.0169\n",
      "Epoch 25/350 Train Loss: 0.9905, Validation Loss: 0.9914 Best Validation Loss: 0.9914\n",
      "Epoch 30/350 Train Loss: 0.9710, Validation Loss: 0.9683 Best Validation Loss: 0.9683\n",
      "Epoch 35/350 Train Loss: 0.9506, Validation Loss: 0.9484 Best Validation Loss: 0.9484\n",
      "Epoch 40/350 Train Loss: 0.9350, Validation Loss: 0.9312 Best Validation Loss: 0.9312\n",
      "Epoch 45/350 Train Loss: 0.9170, Validation Loss: 0.9180 Best Validation Loss: 0.9180\n",
      "Epoch 50/350 Train Loss: 0.9026, Validation Loss: 0.9061 Best Validation Loss: 0.9061\n",
      "Epoch 55/350 Train Loss: 0.8980, Validation Loss: 0.8958 Best Validation Loss: 0.8958\n",
      "Epoch 60/350 Train Loss: 0.8850, Validation Loss: 0.8870 Best Validation Loss: 0.8870\n",
      "Epoch 65/350 Train Loss: 0.8782, Validation Loss: 0.8790 Best Validation Loss: 0.8790\n",
      "Epoch 70/350 Train Loss: 0.8692, Validation Loss: 0.8715 Best Validation Loss: 0.8715\n",
      "Epoch 75/350 Train Loss: 0.8588, Validation Loss: 0.8654 Best Validation Loss: 0.8654\n",
      "Epoch 80/350 Train Loss: 0.8568, Validation Loss: 0.8591 Best Validation Loss: 0.8591\n",
      "Epoch 85/350 Train Loss: 0.8499, Validation Loss: 0.8533 Best Validation Loss: 0.8533\n",
      "Epoch 90/350 Train Loss: 0.8449, Validation Loss: 0.8483 Best Validation Loss: 0.8483\n",
      "Epoch 95/350 Train Loss: 0.8365, Validation Loss: 0.8419 Best Validation Loss: 0.8419\n",
      "Epoch 100/350 Train Loss: 0.8298, Validation Loss: 0.8361 Best Validation Loss: 0.8361\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, val_losses, best_val_loss, time \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                                                  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\DS-AI\\src\\utils\\train.py:47\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     49\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\sagi8\\anaconda3\\envs\\dsaie\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sagi8\\anaconda3\\envs\\dsaie\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, best_val_loss, time = train_and_validate(\n",
    "                                                                   model = model,\n",
    "                                                                   train_loader = train_loader,\n",
    "                                                                   val_loader = val_loader,\n",
    "                                                                   criterion = criterion,\n",
    "                                                                   optimizer = optimizer,\n",
    "                                                                   num_epochs = num_epochs,\n",
    "                                                                   device = device,\n",
    "                                                                   save_path = save_path\n",
    "                                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate_model(model, test_loader, criterion, device=device)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training')\n",
    "plt.plot(val_losses, label='Validation')\n",
    "plt.yscale('log')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one sample\n",
    "data_id = 2 # simulaition number\n",
    "\n",
    "x = test_dataset[data_id][0].unsqueeze(0)\n",
    "WL= test_dataset[data_id][1]\n",
    "\n",
    "print(x.size())\n",
    "print(WL.size())\n",
    "\n",
    "# predict the FAT\n",
    "pred_WL = model(x).detach()\n",
    "\n",
    "demx = x.squeeze(0)[0].reshape(grid_size,grid_size,timestep).permute((2,0,1))\n",
    "WL = WL.squeeze(0).reshape(grid_size,grid_size,timestep).permute((2,0,1))\n",
    "pred_WL = pred_WL.squeeze(0,1).reshape(grid_size,grid_size,timestep).permute((2,0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(demx.size())\n",
    "# print(WL.size())\n",
    "# print(pred_WL.size())\n",
    "\n",
    "print(WL[1] - WL[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "t = 96\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(17,5))\n",
    "diff_WL = WL - pred_WL\n",
    "max_FAT = max(pred_WL.max(), WL.max())\n",
    "max_diff = max(diff_WL.max(), -diff_WL.min())\n",
    "\n",
    "axs[0].imshow(demx[0], cmap='terrain', origin='lower')\n",
    "axs[1].imshow(WL[t] - demx[0], cmap='Blues', origin='lower')\n",
    "axs[2].imshow(pred_WL[t]- demx[0], cmap='Blues', origin='lower')\n",
    "axs[3].imshow(diff_WL[t], cmap='RdBu', origin='lower')\n",
    "\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = demx.min(), vmax=demx.max()),\n",
    "                            cmap='terrain'), fraction=0.05, shrink=0.9, ax=axs[0])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_FAT),\n",
    "                            cmap='Blues'), fraction=0.05, shrink=0.9, ax=axs[1])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_FAT),\n",
    "                            cmap='Blues'), fraction=0.05, shrink=0.9, ax=axs[2])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=TwoSlopeNorm(vmin=-max_diff, vmax=max_diff, vcenter=0),\n",
    "                            cmap='RdBu_r'), fraction=0.05, shrink=0.9, ax=axs[3])\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0].set_title('DEM')\n",
    "axs[1].set_title('Real WD (h)')\n",
    "axs[2].set_title('Predicted WD (h)')\n",
    "axs[3].set_title('Difference (h)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = (64,64)\n",
    "x = np.linspace(0,6350, nx)\n",
    "y = np.linspace(0,6350, ny)\n",
    "xv, yv = np.meshgrid(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import flow_animation\n",
    "pred_WLt = np.array(pred_WL)\n",
    "path = str(src) + \"/animations/Flooding AnimationTempCNN(VX,VY).gif\"\n",
    "flow_animation.animation_create(\n",
    "                 savepath = path,\n",
    "                 X = xv,\n",
    "                 Y = yv,\n",
    "                 Z = test[data_id].topography,\n",
    "                 wd = pred_WLt,\n",
    "                 N = 64,\n",
    "                 fps = 10, color_dem = cm.gist_earth, mask_threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
