{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = pathlib.Path().resolve()\n",
    "src = cwd.parent\n",
    "root = src.parent\n",
    "sys.path.append(str(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.RNN import SimpleRNN\n",
    "from utils.train import train_and_validate, evaluate_model, train\n",
    "from utils.watertopo import WaterTopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA enabled? False\n",
      "Number of GPUs 0\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())\n",
    "print(\"Number of GPUs\",torch.cuda.device_count())\n",
    "print('Using device:', device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data\n",
    "tra_datapath = str(root)+\"/data/normalized_data/tra_val\"\n",
    "test_datapath = str(root)+\"/data/normalized_data/test1\"\n",
    "grid_size = 64\n",
    "sim_amount = 80\n",
    "test_amount = 20\n",
    "\n",
    "# select one simulation for training\n",
    "sims = WaterTopo.load_simulations(save_folder = tra_datapath,\n",
    "                                  sim_amount = sim_amount,\n",
    "                                  number_grids = grid_size)\n",
    "\n",
    "test = WaterTopo.load_simulations(save_folder=test_datapath,\n",
    "                                  sim_amount = test_amount,\n",
    "                                  number_grids = grid_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1,3, figsize = (12,10))\n",
    "# axs[0].imshow(test[0].topography, cmap = 'terrain')\n",
    "# axs[1].imshow(test[0].wd[-1], cmap = 'Blues')\n",
    "# axs[2].quiver(np.arange(0, 64), np.arange(0, 64), test[0].vx[-1], test[0].vy[-1])\n",
    "# axs[2].set_aspect('equal', adjustable='box')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the simulations for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocessing(data, sim_amount, timestep, gridsize):\n",
    "    '''\n",
    "    # Reshape the data into into the 2D array with\n",
    "    # each row representing the pixels (each pixel is regarded as a new variable)\n",
    "    # each column representing the timestep (condider the temporal features as the other dimension of images)\n",
    "    '''\n",
    "    dem = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "    wd = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "    wl = np.zeros((sim_amount,timestep,gridsize,gridsize))\n",
    "\n",
    "    for i in range(sim_amount):\n",
    "        wd[i] =  data[i].wd\n",
    "        dem[i] = data[i].topography\n",
    "        wl[i] = wd[i] + dem[i]\n",
    "\n",
    "    def dataconvert(meshdata,timestep,gridsize):\n",
    "        tempCNN_array = np.zeros((gridsize*gridsize,timestep))\n",
    "        for i in range(timestep):\n",
    "            tempCNN_array[:,i] = meshdata[i].reshape(-1)\n",
    "        return tempCNN_array\n",
    "\n",
    "    dem_new = []\n",
    "    wd_new = []\n",
    "    wl_new = []\n",
    "\n",
    "    for i in range(sim_amount):\n",
    "        dem_new.append(dataconvert(dem[i],timestep,gridsize))\n",
    "        wd_new.append(dataconvert(wd[i],timestep,gridsize))\n",
    "        wl_new.append(dataconvert(wl[i],timestep,gridsize))\n",
    "\n",
    "    return dem_new, wd_new, wl_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = len(sims[0].wd)\n",
    "\n",
    "# Training data\n",
    "dem_new, wd_new, wl_new = dataprocessing(sims, sim_amount = sim_amount, timestep = timestep, gridsize= grid_size)\n",
    "\n",
    "# Testing data\n",
    "dem_test, wd_test, wl_test = dataprocessing(test, sim_amount = test_amount, timestep = timestep, gridsize= grid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# timestep = len(sims[0].wd)\n",
    "# wd = np.zeros((sim_amount,timestep,grid_size,grid_size))\n",
    "# dem = np.zeros((sim_amount,timestep,grid_size,grid_size))\n",
    "# # create waterlavel as the output\n",
    "# wl = np.zeros((sim_amount,timestep,grid_size,grid_size))\n",
    "\n",
    "# for i in range(sim_amount):\n",
    "#     wd[i] =  sims[i].wd\n",
    "#     dem[i] = sims[i].topography\n",
    "#     wl[i] = wd[i] + dem[i]\n",
    "\n",
    "\n",
    "# print(dem.shape)\n",
    "# print(wd.shape)\n",
    "\n",
    "# def dataconvert(meshdata,timestep,gridsize):\n",
    "#     tempCNN_array = np.zeros((gridsize*gridsize,timestep))\n",
    "#     for i in range(timestep):\n",
    "#         tempCNN_array[:,i] = meshdata[i].reshape(-1)\n",
    "#     return tempCNN_array\n",
    "\n",
    "# dem_new = []\n",
    "# wd_new = []\n",
    "# wl_new = []\n",
    "\n",
    "# for i in range(sim_amount):\n",
    "#     dem_new.append(dataconvert(dem[i],timestep,grid_size))\n",
    "#     wd_new.append(dataconvert(wd[i],timestep,grid_size))\n",
    "#     wl_new.append(dataconvert(wl[i],timestep,grid_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the color\n",
    "# color_dem = cm.gist_earth\n",
    "# color_water = 'Blues'\n",
    "\n",
    "# fig, axs = plt.subplots(2,5 , figsize = (12,10))\n",
    "# axs[0,0].imshow(dem_new[0], cmap = color_dem)\n",
    "# axs[0,1].imshow(wd_new[0], cmap = color_water)\n",
    "# axs[0,2].imshow(wl_new[0], cmap = color_dem)\n",
    "# axs[0,3].imshow(vx_new[0])\n",
    "# axs[0,4].imshow(vy_new[0])\n",
    "\n",
    "# axs[1,0].imshow(dem_test[0], cmap = color_dem)\n",
    "# axs[1,1].imshow(wd_test[0], cmap = color_water)\n",
    "# axs[1,2].imshow(wl_test[0], cmap = color_dem)\n",
    "# axs[1,3].imshow(vx_new[0])\n",
    "# axs[1,4].imshow(vy_new[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_tra, dem_val, wd_tra, wd_val, wl_tra, wl_val, ix_tra, ix_tst = train_test_split(\n",
    "    dem_new, wd_new, wl_new, np.arange(sim_amount), test_size=0.30, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the input for the model by combining dem and water depth and the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = np.stack((dem_tra, wd_tra),1)\n",
    "train_outputs = np.array(wl_tra)[:,None]\n",
    "\n",
    "val_inputs = np.stack((dem_val, wd_val),1)\n",
    "val_outputs = np.array(wl_val)[:,None]\n",
    "\n",
    "test_inputs = np.stack((dem_test, wd_test),1)\n",
    "test_outputs = np.array(wl_test)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(train_inputs, dtype=torch.float32), torch.tensor(train_outputs, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(val_inputs, dtype=torch.float32), torch.tensor(val_outputs, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(test_inputs, dtype=torch.float32), torch.tensor(test_outputs, dtype=torch.float32 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Unet Encoder-Decoder CNN to train the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias)]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(num_features=out_channels))\n",
    "        layers.append(nn.PReLU())\n",
    "        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, bias=bias))\n",
    "\n",
    "        self.cnnblock = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnnblock(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=[4, 8, 16], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            outs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return outs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=[16, 8, 4], kernel_size=3, padding=1, bias=False, batch_norm=True):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.upconvs = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(channels[block], channels[block+1], kernel_size=2, padding=0, stride=1)\n",
    "            for block in range(len(channels)-1)]\n",
    "            )\n",
    "        self.dec_blocks = nn.ModuleList([\n",
    "            CNNBlock(channels[block], channels[block+1], kernel_size, padding, bias,\n",
    "                     batch_norm=batch_norm)\n",
    "             for block in range(len(channels)-1)]\n",
    "             )\n",
    "\n",
    "    def forward(self, x, x_skips):\n",
    "        for i in range(len(x_skips)):\n",
    "            x = self.upconvs[i](x)\n",
    "            x = torch.cat((x, x_skips[-(1+i)]), dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "\n",
    "        x = self.dec_blocks[-1](x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, node_features, out_dim=1, n_downsamples=3, initial_hid_dim=64, batch_norm=True,\n",
    "                 bias=True):\n",
    "        super(CNN, self).__init__()\n",
    "        hidden_channels = [initial_hid_dim*2**i for i in range(n_downsamples)]\n",
    "        encoder_channels = [node_features]+hidden_channels\n",
    "        decoder_channels = list(reversed(hidden_channels))+[out_dim]\n",
    "\n",
    "        self.encoder = Encoder(encoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "        self.decoder = Decoder(decoder_channels, kernel_size=3, padding=1,\n",
    "                               bias=bias, batch_norm=batch_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x[-1], x[:-1])\n",
    "        x = nn.Sigmoid()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = 2 # Water Depth and DEM\n",
    "model = CNN(node_features=node_features,\n",
    "            n_downsamples=3,\n",
    "            initial_hid_dim=2,\n",
    "            batch_norm=True, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training parameter, the optimizer, and the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "learning_rate = 0.0005\n",
    "batch_size = 16\n",
    "num_epochs = 150\n",
    "device = 'cpu'\n",
    "# Create the optimizer to train the neural network via back-propagation\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "model_name = 'TempCNN(VX,VY)'\n",
    "save_path = \"../results/trained_models/\" + model_name\n",
    "# Create the training and validation dataloaders to \"feed\" data to the model in batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_losses, val_losses, best_val_loss, time = train_and_validate(\n",
    "#                                                                    model = model,\n",
    "#                                                                    train_loader = train_loader,\n",
    "#                                                                    val_loader = val_loader,\n",
    "#                                                                    criterion = criterion,\n",
    "#                                                                    optimizer = optimizer,\n",
    "#                                                                    num_epochs = num_epochs,\n",
    "#                                                                    device = device,\n",
    "#                                                                    save_path = save_path\n",
    "#                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss = evaluate_model(model, test_loader, criterion, device=device)\n",
    "# print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_losses, label='Training')\n",
    "# plt.plot(val_losses, label='Validation')\n",
    "# plt.yscale('log')\n",
    "# plt.title('Losses')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.legend()\n",
    "# plt.savefig(\"TempCNN Loss.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "saving_model_path = str(src) + \"/results/trained_models/TempCNN\"\n",
    "model.load_state_dict(torch.load(saving_model_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one sample\n",
    "data_id = 2 # simulaition number\n",
    "\n",
    "x = test_dataset[data_id][0].unsqueeze(0)\n",
    "WD= test_dataset[data_id][0][1]\n",
    "\n",
    "# predict WL\n",
    "pred_WL = model(x).detach()\n",
    "\n",
    "demx = x.squeeze(0)[0].reshape(grid_size,grid_size,timestep).permute((2,0,1))\n",
    "WD = WD.reshape(grid_size,grid_size,timestep).permute((2,0,1))\n",
    "\n",
    "pred_WL = pred_WL.squeeze(0,1).reshape(grid_size,grid_size,timestep).permute((2,0,1))\n",
    "\n",
    "pred_WD = pred_WL - demx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "t = 48\n",
    "\n",
    "diff_WD = WD[t] - pred_WD[t]\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(17,5))\n",
    "max_FAT = max(pred_WD.max(), WD.max())\n",
    "max_diff = max(diff_WD.max(), -diff_WD.min())\n",
    "\n",
    "axs[0].imshow(demx[0], cmap='terrain', origin='lower')\n",
    "axs[1].imshow(WD[t], cmap='Blues', origin='lower')\n",
    "axs[2].imshow(pred_WD[t], cmap='Blues', origin='lower')\n",
    "axs[3].imshow(diff_WD, cmap='RdBu', origin='lower')\n",
    "\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = demx.min(), vmax=demx.max()),\n",
    "                            cmap='terrain'), fraction=0.05, shrink=0.9, ax=axs[0])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_FAT),\n",
    "                            cmap='Blues'), fraction=0.05, shrink=0.9, ax=axs[1])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin = 0, vmax=max_FAT),\n",
    "                            cmap='Blues'), fraction=0.05, shrink=0.9, ax=axs[2])\n",
    "plt.colorbar(plt.cm.ScalarMappable(norm=TwoSlopeNorm(vmin=-max_diff, vmax=max_diff, vcenter=0),\n",
    "                            cmap='RdBu'), fraction=0.05, shrink=0.9, ax=axs[3])\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "axs[0].set_title('DEM')\n",
    "axs[1].set_title('Real WD')\n",
    "axs[2].set_title('Predicted WD')\n",
    "axs[3].set_title('Difference (RealWd - PredWD)')\n",
    "plt.savefig(\"TempCNN Prediction.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Performance import Performance\n",
    "from matplotlib.animation import ArtistAnimation, PillowWriter\n",
    "\n",
    "\n",
    "save_path_accuracy = \"../results/trained_models/\" + \"/accuracy.gif\"\n",
    "# Performance(pred_WD, WD, 'animation', save_path = save_path_accuracy)\n",
    "Performance(pred_WD, WD, 'specific', timestep = 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to generate the animation\n",
    "# nx, ny = (64,64)\n",
    "# x = np.linspace(0,6350, nx)\n",
    "# y = np.linspace(0,6350, ny)\n",
    "# xv, yv = np.meshgrid(x,y)\n",
    "# from utils import flow_animation\n",
    "# pred_WLt = np.array(pred_WL)\n",
    "# path = str(src) + \"/animations/Flooding AnimationTempCNN.gif\"\n",
    "# flow_animation.animation_create(\n",
    "#                  savepath = path,\n",
    "#                  X = xv,\n",
    "#                  Y = yv,\n",
    "#                  Z = test[data_id].topography,\n",
    "#                  wd = pred_WLt,\n",
    "#                  N = 64,\n",
    "#                  fps = 10, color_dem = cm.gist_earth, mask_threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import count_parameters, mse_per_timestep\n",
    "# This cell saves the averaged MSE for each timestep for 20 test simulations\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(str(src)+\"/results/trained_models/Tempcnn/TempCNN\"))\n",
    "\n",
    "grid_size = 64\n",
    "channels = 2\n",
    "sim_length = 97\n",
    "\n",
    "# Let's do a crude Monte Carlo with a single model, calculating the loss after each timestep\n",
    "sims = WaterTopo.load_simulations(str(root)+\"/data/normalized_data/test1/\", 20, grid_size, use_augmented_data=True)\n",
    "mse = np.zeros(97)\n",
    "\n",
    "for sim in sims:\n",
    "\n",
    "   inputs = np.zeros((1, channels, grid_size, grid_size))\n",
    "   targets = sim.wd\n",
    "\n",
    "\n",
    "   # Select the time step where you want to start\n",
    "   id = 0\n",
    "\n",
    "   inputs[0, 0, :, :] = sim.topography\n",
    "   inputs[0, 1, :, :] = sim.return_timestep(id)\n",
    "   inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "\n",
    "   targets = torch.tensor(targets, dtype=torch.float32)\n",
    "   targets[0,:,:] = inputs[0,1,:,:]\n",
    "\n",
    "   outputs = torch.zeros(targets.shape)\n",
    "   outputs[0,:,:] = inputs[0,1,:,:]\n",
    "\n",
    "   for t in range(1, sim_length):\n",
    "      outputs[t,:,:] = model(inputs.to(device)).detach()\n",
    "      inputs[0,1,:,:] = outputs[t,:,:]\n",
    "\n",
    "   mse += mse_per_timestep(targets, outputs)\n",
    "\n",
    "mse = mse / len(sims)\n",
    "\n",
    "np.savetxt(str(src)+\"/results/error_accumulation/TempCNN\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
